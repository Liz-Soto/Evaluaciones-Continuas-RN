{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imagenes/rn3.png\" width=\"200\">\n",
    "<img src=\"http://www.identidadbuho.uson.mx/assets/letragrama-rgb-150.jpg\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [Curso de Redes Neuronales](https://curso-redes-neuronales-unison.github.io/Temario/)\n",
    "\n",
    "# Una red neuronal multicapa simple usando TensorFlow\n",
    "\n",
    "\n",
    "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/), 27 de septiembre de 2017.\n",
    "\n",
    "\n",
    "\n",
    "En esta libreta se muestra el ejemplo básico para una red multicapa sencilla\n",
    "aplicada al conjunto de datos [MNIST](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "Esta libreta es básicamente una traducción del ejemplo\n",
    "desarrollado por [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar datos\n",
    "\n",
    "Primero cargamos los archivos que se utilizan para el aprendizaje. Para otro tipo de problemas, es necesario hacer un proceso conocido como *Data Wrangling*, que normalmente se realiza con la ayuda de *Pandas*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-c3d55fec490c>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\sotof\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\sotof\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\sotof\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\sotof\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\sotof\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\sotof\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que un aprendizaje tenga sentido es necesario tener bien separado un conjunto de datos de aprendizaje y otro de prueba (en caso de grandes conjuntos de datos es la opción). Como vemos tanto las imágenes como las etiquetas están separados en archivos de datos y de aprendizaje.\n",
    "\n",
    "El objeto `mnist` es un objeto tensorflow que contiene 3 objetos tipo tensorflow: *test*, *train* y *validation*, los cuales a su vez contienen *ndarrays* de *numpy*. La estructura es la misma para cada conjunto de datos. Veamos su estructura:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo de images: <class 'numpy.ndarray'>\n",
      "Tipo de epochs_completed: <class 'int'>\n",
      "Tipo de labels: <class 'numpy.ndarray'>\n",
      "Tipo de nest_batch: <class 'method'>\n",
      "Tipo de num_examples: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Tipo de images: {}\".format(type(mnist.train.images)))\n",
    "print(\"Tipo de epochs_completed: {}\".format(type(mnist.train.epochs_completed)))\n",
    "print(\"Tipo de labels: {}\".format(type(mnist.train.labels)))\n",
    "print(\"Tipo de nest_batch: {}\".format(type(mnist.train.next_batch)))\n",
    "print(\"Tipo de num_examples: {}\".format(type(mnist.train.num_examples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como generar el conjunto de datos para ser utilizado dentro de TensorFlow es objeto de otra libreta. Por el momento concentremonos en como hacer una red neuronal rápido y sin dolor.\n",
    "\n",
    "Sin embargo, vamos a ver unos cuantos datos que nos pueden ser de útilidad para la construcción de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del ndarray con las imágenes: (55000, 784)\n",
      "Forma del ndarray con las etiquetas: (55000, 10)\n",
      "-------------------------------------------------------------------------------\n",
      "Número de imagenes de entrenamiento: 55000\n",
      "Tamaño de las imagenes: 784\n",
      "Clases diferentes: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Forma del ndarray con las imágenes: {}\".format(mnist.train.images.shape))\n",
    "print(\"Forma del ndarray con las etiquetas: {}\".format(mnist.train.labels.shape))\n",
    "print(\"-\" * 79)\n",
    "print(\"Número de imagenes de entrenamiento: {}\".format(mnist.train.images.shape[0]))\n",
    "print(\"Tamaño de las imagenes: {}\".format(mnist.train.images.shape[1]))\n",
    "print(\"Clases diferentes: {}\".format(mnist.train.labels.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construcción de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer una red neuronal lo más genérica posible y que pdamos reutilizar en otros proyectos, vamos a establecer los parámetros base independientemente de la inicialización de la red, independientemente de la forma en que construimos la red. \n",
    "\n",
    "Comencemos por establecer una función genérica que nos forme una red neuronal con dos capas ocultas. No agrego más comentarios porque, con la experiencia de las libretas anteriores, la construcción de la red neuronal se explica sola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_neuronal_dos_capas_ocultas(x, pesos, sesgos):\n",
    "    \"\"\"\n",
    "    Genera una red neuronal de dos capas para usar en TensorFlow\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    pesos: un diccionario con tres etiquetas: 'h1', 'h2' y 'ho'\n",
    "           en donde cada una es una tf.Variable conteniendo una \n",
    "           matriz de dimensión [num_neuronas_capa_anterior, num_neuronas_capa]\n",
    "                  \n",
    "    sesgos: un diccionario con tres etiquetas: 'b1', 'b2' y 'bo'\n",
    "            en donde cada una es una tf.Variable conteniendo un\n",
    "            vector de dimensión [numero_de_neuronas_capa]\n",
    "                   \n",
    "    Devuelve\n",
    "    --------\n",
    "    Un ops de tensorflow que calcula la salida de una red neuronal\n",
    "    con dos capas ocultas, y activaciones RELU.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Primera capa oculta con activación ReLU\n",
    "    capa_1 = tf.matmul(x, pesos['h1'])\n",
    "    capa_1 = tf.add(capa_1, sesgos['b1'])\n",
    "    capa_1 = tf.nn.relu(capa_1)\n",
    "    \n",
    "    # Segunda capa oculta con activación ReLU\n",
    "    capa_2 = tf.matmul(capa_1, pesos['h2'])\n",
    "    capa_2 = tf.add(capa_2, sesgos['b2'])\n",
    "    capa_2 = tf.nn.relu(capa_2)\n",
    "    \n",
    "    # Capa de salida con activación lineal\n",
    "    # En Tensorflow la salida es siempre lineal, y luego se especifica\n",
    "    # la función de salida a la hora de calcularla como vamos a ver \n",
    "    # más adelante\n",
    "    capa_salida = tf.matmul(capa_2, pesos['ho']) + sesgos['bo']\n",
    "    return capa_salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora necesitamos poder generar los datos de entrada a la red neuronal de\n",
    "alguna manera posible. Afortunadamente sabemos exactamente que necesitaos, así\n",
    "que vamos a hacer una función que nos genere las variables de peso y sesgo.\n",
    "\n",
    "Por el momento, y muy a la brava, solo vamos a generarlas con números aletorios con una \n",
    "distribución $\\mathcal{N}(0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializa_pesos(entradas, n1, n2, salidas):\n",
    "    \"\"\"\n",
    "    Genera un diccionario con pesos  \n",
    "    para ser utilizado en la función red_neuronal_dos_capas_ocultas\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    entradas: Número de neuronas en la capa de entrada\n",
    "    \n",
    "    n1: Número de neuronas en la primer capa oculta\n",
    "    \n",
    "    n2: Número de neuronas en la segunda capa oculta\n",
    "    \n",
    "    salidas: Número de neuronas de salida\n",
    "    \n",
    "    Devuelve\n",
    "    --------\n",
    "    Dos diccionarios, uno con los pesos por capa y otro con los sesgos por capa\n",
    "    \n",
    "    \"\"\"\n",
    "    pesos = {\n",
    "        'h1': tf.Variable(tf.random_normal([entradas, n1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n1, n2])),\n",
    "        'ho': tf.Variable(tf.random_normal([n2, salidas]))\n",
    "    }\n",
    "    \n",
    "    sesgos = {\n",
    "        'b1': tf.Variable(tf.random_normal([n1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n2])),\n",
    "        'bo': tf.Variable(tf.random_normal([salidas]))\n",
    "    }\n",
    "    \n",
    "    return pesos, sesgos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos establecer los parámetros de la topología de la red neuronal. \n",
    "Tomemos en cuenta que estos prámetros los podríamos haber establecido desde\n",
    "la primer celda, si el fin es estar variando los parámetros para escoger los que \n",
    "ofrezcan mejor desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entradas = 784  #  Lo sabemos por la inspección que hicimos a mnist\n",
    "num_salidas = 10    # Ídem\n",
    "\n",
    "# Aqui es donde podemos jugar\n",
    "num_neuronas_capa_1 = 256\n",
    "num_neuronas_capa_2 = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡A construir la red! Para esto vamos a necesitar crear las entradas\n",
    "con un placeholder, y crear nuestra topología de red neuronal.\n",
    "\n",
    "Observa que la dimensión de x será [None, num_entradas], lo que significa que \n",
    "la cantidad de renglones es desconocida (o variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sotof\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# La entrada a la red neuronal\n",
    "x = tf.placeholder(\"float\", [None, num_entradas])\n",
    "\n",
    "# Los pesos y los sesgos\n",
    "w, b = inicializa_pesos(num_entradas, num_neuronas_capa_1, num_neuronas_capa_2, num_salidas)\n",
    "\n",
    "# Crea la red neuronal\n",
    "estimado = red_neuronal_dos_capas_ocultas(x, w, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parecería que ya está todo listo. Sin ambargo falta algo muy importante: No hemos explicado\n",
    "ni cual es el criterio de error (loss) que vamos a utilizar, ni cual va a ser el método de\n",
    "optimización (aprendizaje) que hemos decidido aplicar.\n",
    "\n",
    "Primero definamos el costo que queremos minimizar, y ese costo va a estar en función de lo\n",
    "estimado con lo real, por lo que necesitamos otra entrada de datos para los datos de salida.\n",
    "\n",
    "Sin ningun lugar a dudas, el costo que mejor describe este problema es el de *softmax*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-86b9e8f4cd9d>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Creamos la variable de datos de salida conocidos\n",
    "y = tf.placeholder(\"float\", [None, num_salidas])\n",
    "\n",
    "#  Definimos la función de costo\n",
    "costo = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=estimado, labels=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora definimos que función de aprendizaje vamos a utilizar. Existen muchas funciones\n",
    "de aprendizaje en tensorflow, las cuales se pueden consultar en `tf.train.`. Entre las\n",
    "existentes podemos ver algunas conocidas del curso como descenso de gradiente simple,\n",
    "momento, rprop, rmsprop entre otras. Casi todas las funciones de optimización (aprendizaje)\n",
    "acaban su nombre con `Optimize`.\n",
    "\n",
    "En este caso vamos a usar un método comocido como el *algoritmo de Adam* el cual \n",
    "se puede consultar [aqui](http://arxiv.org/pdf/1412.6980.pdf). El metodo utiliza dos calculos\n",
    "de momentos diferentes, y por lo visto genera resultados muy interesantes desde el punto \n",
    "de vista práctico.\n",
    "\n",
    "¿Cual es el mejor método? Pues esto es en función de tu problema y de la cantidad de datos que tengas.\n",
    "Lo mejor es practicar con varios métodos para entender sus ventajas y desventajas.\n",
    "\n",
    "En todo caso el método de optimización requiere que se le inicialice con una tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alfa = 0.001\n",
    "optimizador = tf.train.AdamOptimizer(learning_rate=alfa)\n",
    "paso_entrenamiento = optimizador.minimize(costo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ejecutar la sesión usando mini-batches\n",
    "\n",
    "Ahora, ya que la red neuronal está lista vamos a ejecutar la red utilizando el algoritmo de\n",
    "Adam pero en forma de mini-batches. Con el fin de tener control sobre el problema, vamos a establecer un número máximo de epoch (ciclos de aprendizaje), el tamaño de los mini-batches, y cada cuandos epoch \n",
    "quisieramos ver como está evolucionando la red neuronal.\n",
    "\n",
    "Como entrenar una red neuronal no tiene sentido, si no es porque la queremos usar para reconocer,\n",
    "no tendría sentido entrenarla y luego perderla y tener que reentrenar en cada ocasión. Recuerda que cuando\n",
    "se cierra la sesión se borra todo lo que se tenía en memoria. \n",
    "\n",
    "Para esto vamos a usar una ops especial llamada `Saver`, que permite guardar en un archivo la red neuronal y \n",
    "después utilizarla en otra sesión (en otro script, computadora, ....).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_modelo = \"rnn2.ckpt\"\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como todo se ejecuta dentro de una sesión, no es posible hacerlo por partes (si usamos el \n",
    "`with` que debería ser la única forma en la que iniciaramos una sesión). Por lo tanto procuraré dejar comentado el código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0            Costo: 160.83733494151724\n",
      "Epoch: 1            Costo: 41.71173904765732\n",
      "Epoch: 2            Costo: 26.339589040062645\n",
      "Epoch: 3            Costo: 18.597581534385686\n",
      "Epoch: 4            Costo: 13.631360631659858\n",
      "Epoch: 5            Costo: 10.124610042969206\n",
      "Epoch: 6            Costo: 7.723124431487444\n",
      "Epoch: 7            Costo: 5.741367376846774\n",
      "Epoch: 8            Costo: 4.3009156647635205\n",
      "Epoch: 9            Costo: 3.2330499522703855\n",
      "Epoch: 10           Costo: 2.5650033133265335\n",
      "Epoch: 11           Costo: 1.92610898914332\n",
      "Epoch: 12           Costo: 1.4684379593541275\n",
      "Epoch: 13           Costo: 1.1741105068786926\n",
      "Epoch: 14           Costo: 0.9336911004276206\n",
      "Epoch: 15           Costo: 0.7696978474093763\n",
      "Epoch: 16           Costo: 0.7385285291160316\n",
      "Epoch: 17           Costo: 0.57963557503229\n",
      "Epoch: 18           Costo: 0.5225207725525219\n",
      "Epoch: 19           Costo: 0.5253588119058014\n",
      "Epoch: 20           Costo: 0.49752028947470495\n",
      "Epoch: 21           Costo: 0.4525364053871059\n",
      "Epoch: 22           Costo: 0.3868440303907707\n",
      "Epoch: 23           Costo: 0.3146025881040694\n",
      "Epoch: 24           Costo: 0.32150707179814253\n",
      "Epoch: 25           Costo: 0.3878006620298616\n",
      "Epoch: 26           Costo: 0.2887802479758159\n",
      "Epoch: 27           Costo: 0.23906038654358516\n",
      "Epoch: 28           Costo: 0.2044979180715289\n",
      "Epoch: 29           Costo: 0.3725342431540081\n",
      "Se acabaron los epochs, saliendo de la sesión de tensorflow.\n"
     ]
    }
   ],
   "source": [
    "numero_epochs = 30\n",
    "tamano_minibatch = 100\n",
    "display_step = 1\n",
    "\n",
    "# Muy importante la primera vez que se ejecuta inicializar todas las variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# La manera correcta de iniciar una sesión y realizar calculos\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Ciclos de entrenamiento\n",
    "    for epoch in range(numero_epochs):\n",
    "\n",
    "        #  Inicializa el costo promedio de todos los minibatches en 0\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        #  Calcula el número de minibatches que se pueden usar \n",
    "        total_batch = int(mnist.train.num_examples/tamano_minibatch)\n",
    "\n",
    "        #  Por cada minibatch\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            #  Utiliza un generador incluido en mnist que obtiene \n",
    "            #  tamano_minibatch ejemplos selecionados aleatoriamente del total\n",
    "            batch_x, batch_y = mnist.train.next_batch(tamano_minibatch)\n",
    "            \n",
    "            #  Ejecuta la ops del paso_entrenamiento para aprender \n",
    "            #  y la del costo, con el fin de mostrar el aprendizaje\n",
    "            _, c = sess.run([paso_entrenamiento, costo], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            #  Calcula el costo del minibatch y lo agrega al costo total\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        # Muestra los resultados\n",
    "        if epoch % display_step == 0:\n",
    "            print ((\"Epoch: \" + str(epoch)).ljust(20)\n",
    "                   + (\"Costo: \" + str(avg_cost)))\n",
    "    \n",
    "    #  Guarda la sesión en el archivo rnn2.cptk\n",
    "    saver.save(sess, archivo_modelo)\n",
    "    \n",
    "    print(\"Se acabaron los epochs, saliendo de la sesión de tensorflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ahora vamos a revisar que tan bien realizó el aprendizaje cuando se aplica la red adatos que\n",
    "no se usaron para entrenamiento. Para esto vamos a utilizar dos ops extas: una \n",
    "para definir la operaración de datos bien estimados o mal estimados, y otra para\n",
    "calcular el promedio de datos bien estimados. Para calcular los datos bien estimados vamos a utilizar `tf.cast` que permite ajustar los tipos\n",
    "al tipo tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_correcta = tf.equal(tf.argmax(estimado, 1), tf.argmax(y, 1))\n",
    "\n",
    "precision = tf.reduce_mean(tf.cast(prediction_correcta, \"float\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si, vamos a abrir una nueva sesión, vamos a restaurar los valores de la sesión anterior,\n",
    "y vamos a ejecutar el grafo con el fin de evaluar la ops precision, pero ahora con el\n",
    "diccionario de alimentación con los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sotof\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from rnn2.ckpt\n",
      "Precisión: 0.9584000110626221\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, archivo_modelo)\n",
    "    porcentaje_acierto = sess.run(precision, feed_dict={x: mnist.test.images,\n",
    "                                                        y: mnist.test.labels})\n",
    "    print(\"Precisión: {}\".format(porcentaje_acierto))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Contesta las siguientes preguntas\n",
    "\n",
    "1. ¿Que pasa si aumenta el número de epochs? ¿Cuando deja de ser util aumentar los epoch?  \n",
    "  Va aumentando la precisión, deja de ser util cuando comienza a bajar, que en mis pruebas fue al rededor de 175 epochs\n",
    "\n",
    "2. ¿Que pasa si aumentas o disminuyes la tasa de aprendizaje?  \n",
    "Aumentando la tasa de aprendizaje el costo disminuye más rapido, sin embargo, si es muy alta, deja de converger dando una precisión muy mala.\n",
    "Disminuyendo la taza de aprendizaje converge más despacio.\n",
    "\n",
    "\n",
    "3. Utiliza al menos otros 2 métodos de optimización (existentes en Tensorflow), ajustalos y comparalos. ¿Cual de los métodos te gusta más y porque preferirías unos sobre otros?  \n",
    "El AdamOptimizer me dio el mejor resultado en el mejor tiempo.\n",
    "\n",
    "4. ¿Que pasa si cambias el tamaño de los minibatches?\n",
    "5. ¿Como harías si dejaste a medias un proceso de aprendizaje (en 10 epochs por ejemplo) y quisieras entrenar la red 10 epoch más, y mañana quisieras entrenarla otros 10 epoch más?  \n",
    "Guardar el modelo.\n",
    "\n",
    "**Para contestar las preguntas, agrega cuantas celdas con comentarios y con códgo sean necesarias.** Aprovecha que las libretas de *Jupyter* te permite hacerte una especie de tutorial personalizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0            Costo: 162.79438453674314\n",
      "Epoch: 1            Costo: 40.19226976047864\n",
      "Epoch: 2            Costo: 25.616192274527116\n",
      "Epoch: 3            Costo: 17.930013914325006\n",
      "Epoch: 4            Costo: 13.239954518784167\n",
      "Epoch: 5            Costo: 9.847955660340991\n",
      "Epoch: 6            Costo: 7.508212439054253\n",
      "Epoch: 7            Costo: 5.580312634984546\n",
      "Epoch: 8            Costo: 4.2541547291619\n",
      "Epoch: 9            Costo: 3.1981649510210306\n",
      "Epoch: 10           Costo: 2.4617553194316883\n",
      "Epoch: 11           Costo: 2.0157561061705187\n",
      "Epoch: 12           Costo: 1.4198508812438277\n",
      "Epoch: 13           Costo: 1.1831781622287671\n",
      "Epoch: 14           Costo: 0.9883424979616886\n",
      "Epoch: 15           Costo: 0.7602920162358346\n",
      "Epoch: 16           Costo: 0.6982391424481048\n",
      "Epoch: 17           Costo: 0.6395305333745769\n",
      "Epoch: 18           Costo: 0.5517198173148719\n",
      "Epoch: 19           Costo: 0.40668263780887476\n",
      "Epoch: 20           Costo: 0.4975058415454841\n",
      "Epoch: 21           Costo: 0.3561453901954588\n",
      "Epoch: 22           Costo: 0.38234081100768796\n",
      "Epoch: 23           Costo: 0.3007141553872449\n",
      "Epoch: 24           Costo: 0.3822490163112659\n",
      "Epoch: 25           Costo: 0.2726668040097577\n",
      "Epoch: 26           Costo: 0.36403619527256126\n",
      "Epoch: 27           Costo: 0.27110337213444674\n",
      "Epoch: 28           Costo: 0.23292485795346066\n",
      "Epoch: 29           Costo: 0.34337418148230164\n",
      "Epoch: 30           Costo: 0.27275463172299613\n",
      "Epoch: 31           Costo: 0.2489631637706173\n",
      "Epoch: 32           Costo: 0.27702073474737826\n",
      "Epoch: 33           Costo: 0.2303396774080113\n",
      "Epoch: 34           Costo: 0.21622102715565125\n",
      "Epoch: 35           Costo: 0.1923296584783025\n",
      "Epoch: 36           Costo: 0.19477569143626738\n",
      "Epoch: 37           Costo: 0.24002156208775804\n",
      "Epoch: 38           Costo: 0.21741268132992383\n",
      "Epoch: 39           Costo: 0.18137272869395465\n",
      "Epoch: 40           Costo: 0.18465263792466474\n",
      "Epoch: 41           Costo: 0.16923359764144802\n",
      "Epoch: 42           Costo: 0.18284528285874194\n",
      "Epoch: 43           Costo: 0.1945315741470266\n",
      "Epoch: 44           Costo: 0.19770532849039732\n",
      "Epoch: 45           Costo: 0.1633155302709272\n",
      "Epoch: 46           Costo: 0.1798236777127993\n",
      "Epoch: 47           Costo: 0.16400079855957744\n",
      "Epoch: 48           Costo: 0.22395494735398494\n",
      "Epoch: 49           Costo: 0.15128794770110315\n",
      "Epoch: 50           Costo: 0.1600315208723228\n",
      "Epoch: 51           Costo: 0.15733599225822834\n",
      "Epoch: 52           Costo: 0.16263949218888388\n",
      "Epoch: 53           Costo: 0.08334468520865305\n",
      "Epoch: 54           Costo: 0.17399817652586366\n",
      "Epoch: 55           Costo: 0.14806132659114488\n",
      "Epoch: 56           Costo: 0.12443841605794913\n",
      "Epoch: 57           Costo: 0.15865851829051084\n",
      "Epoch: 58           Costo: 0.12149371956172099\n",
      "Epoch: 59           Costo: 0.1398532390634531\n",
      "Epoch: 60           Costo: 0.12551756920828522\n",
      "Epoch: 61           Costo: 0.14501124033553478\n",
      "Epoch: 62           Costo: 0.1317053816649027\n",
      "Epoch: 63           Costo: 0.11417448305535542\n",
      "Epoch: 64           Costo: 0.1603641688439748\n",
      "Epoch: 65           Costo: 0.11580966605793272\n",
      "Epoch: 66           Costo: 0.14683842362042784\n",
      "Epoch: 67           Costo: 0.11675879157666734\n",
      "Epoch: 68           Costo: 0.11126003186546739\n",
      "Epoch: 69           Costo: 0.11486692978068078\n",
      "Epoch: 70           Costo: 0.1282669380815569\n",
      "Epoch: 71           Costo: 0.1509559181452794\n",
      "Epoch: 72           Costo: 0.09170147060432557\n",
      "Epoch: 73           Costo: 0.08521950847356785\n",
      "Epoch: 74           Costo: 0.1530732354776576\n",
      "Epoch: 75           Costo: 0.10608395027576234\n",
      "Epoch: 76           Costo: 0.10436284703424145\n",
      "Epoch: 77           Costo: 0.1209764521238342\n",
      "Epoch: 78           Costo: 0.12540143241315557\n",
      "Epoch: 79           Costo: 0.09017714593678756\n",
      "Epoch: 80           Costo: 0.0853847093876063\n",
      "Epoch: 81           Costo: 0.10058438629387613\n",
      "Epoch: 82           Costo: 0.07621910388933578\n",
      "Epoch: 83           Costo: 0.08307150381539395\n",
      "Epoch: 84           Costo: 0.13839636988461285\n",
      "Epoch: 85           Costo: 0.10202934583917478\n",
      "Epoch: 86           Costo: 0.12718965854444877\n",
      "Epoch: 87           Costo: 0.07711326980193087\n",
      "Epoch: 88           Costo: 0.08110815308096103\n",
      "Epoch: 89           Costo: 0.1350931320932816\n",
      "Epoch: 90           Costo: 0.07444912990735786\n",
      "Epoch: 91           Costo: 0.11401628701973633\n",
      "Epoch: 92           Costo: 0.11411563906136893\n",
      "Epoch: 93           Costo: 0.05349268946045743\n",
      "Epoch: 94           Costo: 0.04658872595378174\n",
      "Epoch: 95           Costo: 0.15016152086303688\n",
      "Epoch: 96           Costo: 0.13768520574725068\n",
      "Epoch: 97           Costo: 0.09876787231086481\n",
      "Epoch: 98           Costo: 0.10899326357838261\n",
      "Epoch: 99           Costo: 0.11041679381085905\n",
      "Epoch: 100          Costo: 0.05666974920666851\n",
      "Epoch: 101          Costo: 0.11655059117896256\n",
      "Epoch: 102          Costo: 0.052550905421029276\n",
      "Epoch: 103          Costo: 0.08078230222635539\n",
      "Epoch: 104          Costo: 0.04920040021880529\n",
      "Epoch: 105          Costo: 0.10058906201964002\n",
      "Epoch: 106          Costo: 0.12790161031603792\n",
      "Epoch: 107          Costo: 0.09543287598149988\n",
      "Epoch: 108          Costo: 0.06219405704124162\n",
      "Epoch: 109          Costo: 0.10029127878504776\n",
      "Epoch: 110          Costo: 0.07085297010055619\n",
      "Epoch: 111          Costo: 0.0676578420319241\n",
      "Epoch: 112          Costo: 0.06664866435582419\n",
      "Epoch: 113          Costo: 0.09314925749264077\n",
      "Epoch: 114          Costo: 0.10674003479513382\n",
      "Epoch: 115          Costo: 0.05884305115169806\n",
      "Epoch: 116          Costo: 0.0863506619487427\n",
      "Epoch: 117          Costo: 0.10343666620204754\n",
      "Epoch: 118          Costo: 0.07514857927501525\n",
      "Epoch: 119          Costo: 0.046102271341926834\n",
      "Epoch: 120          Costo: 0.08641631451210482\n",
      "Epoch: 121          Costo: 0.05685114459134675\n",
      "Epoch: 122          Costo: 0.08203044837629084\n",
      "Epoch: 123          Costo: 0.06403468671830487\n",
      "Epoch: 124          Costo: 0.10234498012280566\n",
      "Epoch: 125          Costo: 0.09530389432360585\n",
      "Epoch: 126          Costo: 0.0865104213399092\n",
      "Epoch: 127          Costo: 0.07868153611885371\n",
      "Epoch: 128          Costo: 0.05273839921920705\n",
      "Epoch: 129          Costo: 0.03569365650694874\n",
      "Epoch: 130          Costo: 0.11542696504121082\n",
      "Epoch: 131          Costo: 0.06836224340342663\n",
      "Epoch: 132          Costo: 0.06281066917284829\n",
      "Epoch: 133          Costo: 0.06040037309264847\n",
      "Epoch: 134          Costo: 0.08244721960993495\n",
      "Epoch: 135          Costo: 0.11575357600522583\n",
      "Epoch: 136          Costo: 0.09338308417866473\n",
      "Epoch: 137          Costo: 0.06415354654219167\n",
      "Epoch: 138          Costo: 0.055484557602045986\n",
      "Epoch: 139          Costo: 0.07214025651176144\n",
      "Epoch: 140          Costo: 0.06743711505437051\n",
      "Epoch: 141          Costo: 0.08197222710574428\n",
      "Epoch: 142          Costo: 0.052198754666622514\n",
      "Epoch: 143          Costo: 0.055180299500000904\n",
      "Epoch: 144          Costo: 0.07753238454605071\n",
      "Epoch: 145          Costo: 0.03675477130173866\n",
      "Epoch: 146          Costo: 0.10036716527830879\n",
      "Epoch: 147          Costo: 0.11458651748820728\n",
      "Epoch: 148          Costo: 0.07947365229791832\n",
      "Epoch: 149          Costo: 0.059018296724584245\n",
      "Epoch: 150          Costo: 0.0778075336650186\n",
      "Epoch: 151          Costo: 0.043869451737028727\n",
      "Epoch: 152          Costo: 0.05925972498582151\n",
      "Epoch: 153          Costo: 0.07732355617951453\n",
      "Epoch: 154          Costo: 0.05871748101833482\n",
      "Epoch: 155          Costo: 0.0836887878072269\n",
      "Epoch: 156          Costo: 0.07011508511038332\n",
      "Epoch: 157          Costo: 0.06272522876219155\n",
      "Epoch: 158          Costo: 0.0621561057373582\n",
      "Epoch: 159          Costo: 0.028809486843832516\n",
      "Epoch: 160          Costo: 0.05751524649387881\n",
      "Epoch: 161          Costo: 0.06398254667130646\n",
      "Epoch: 162          Costo: 0.054379411580846373\n",
      "Epoch: 163          Costo: 0.07491062483752861\n",
      "Epoch: 164          Costo: 0.05950219841218567\n",
      "Epoch: 165          Costo: 0.04569174170757265\n",
      "Epoch: 166          Costo: 0.03925476847458332\n",
      "Epoch: 167          Costo: 0.08160671051885518\n",
      "Epoch: 168          Costo: 0.08698219940826064\n",
      "Epoch: 169          Costo: 0.06467077784144334\n",
      "Epoch: 170          Costo: 0.06629404175700893\n",
      "Epoch: 171          Costo: 0.05300421906061057\n",
      "Epoch: 172          Costo: 0.06770603042389343\n",
      "Epoch: 173          Costo: 0.04086735119717559\n",
      "Epoch: 174          Costo: 0.11119315310747138\n",
      "Se acabaron los epochs, saliendo de la sesión de tensorflow.\n",
      "INFO:tensorflow:Restoring parameters from rnn2.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión: 0.9731000065803528\n"
     ]
    }
   ],
   "source": [
    "#Incrementando el número de epochs\n",
    "alfa = 0.001\n",
    "optimizador = tf.train.AdamOptimizer(learning_rate=alfa)\n",
    "paso_entrenamiento = optimizador.minimize(costo)\n",
    "\n",
    "archivo_modelo = \"rnn2.ckpt\"\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "numero_epochs = 175\n",
    "tamano_minibatch = 100\n",
    "display_step = 1\n",
    "\n",
    "# Muy importante la primera vez que se ejecuta inicializar todas las variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# La manera correcta de iniciar una sesión y realizar calculos\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Ciclos de entrenamiento\n",
    "    for epoch in range(numero_epochs):\n",
    "\n",
    "        #  Inicializa el costo promedio de todos los minibatches en 0\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        #  Calcula el número de minibatches que se pueden usar \n",
    "        total_batch = int(mnist.train.num_examples/tamano_minibatch)\n",
    "\n",
    "        #  Por cada minibatch\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            #  Utiliza un generador incluido en mnist que obtiene \n",
    "            #  tamano_minibatch ejemplos selecionados aleatoriamente del total\n",
    "            batch_x, batch_y = mnist.train.next_batch(tamano_minibatch)\n",
    "            \n",
    "            #  Ejecuta la ops del paso_entrenamiento para aprender \n",
    "            #  y la del costo, con el fin de mostrar el aprendizaje\n",
    "            _, c = sess.run([paso_entrenamiento, costo], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            #  Calcula el costo del minibatch y lo agrega al costo total\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        # Muestra los resultados\n",
    "        if epoch % display_step == 0:\n",
    "            print ((\"Epoch: \" + str(epoch)).ljust(20)\n",
    "                   + (\"Costo: \" + str(avg_cost)))\n",
    "    \n",
    "    #  Guarda la sesión en el archivo rnn2.cptk\n",
    "    saver.save(sess, archivo_modelo)\n",
    "    \n",
    "    print(\"Se acabaron los epochs, saliendo de la sesión de tensorflow.\")\n",
    "\n",
    "prediction_correcta = tf.equal(tf.argmax(estimado, 1), tf.argmax(y, 1))\n",
    "\n",
    "precision = tf.reduce_mean(tf.cast(prediction_correcta, \"float\"))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, archivo_modelo)\n",
    "    porcentaje_acierto = sess.run(precision, feed_dict={x: mnist.test.images,\n",
    "                                                        y: mnist.test.labels})\n",
    "    print(\"Precisión: {}\".format(porcentaje_acierto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0            Costo: 40.43866363969715\n",
      "Epoch: 1            Costo: 0.9745015419071368\n",
      "Epoch: 2            Costo: 0.8024116824973706\n",
      "Epoch: 3            Costo: 0.7408012174476276\n",
      "Epoch: 4            Costo: 0.5359023708917882\n",
      "Epoch: 5            Costo: 0.5150971717726105\n",
      "Epoch: 6            Costo: 0.4755817582932387\n",
      "Epoch: 7            Costo: 0.46872062595053104\n",
      "Epoch: 8            Costo: 0.46760605950247147\n",
      "Epoch: 9            Costo: 0.6404999000646853\n",
      "Epoch: 10           Costo: 0.801818022023548\n",
      "Epoch: 11           Costo: 1.041679102940992\n",
      "Epoch: 12           Costo: 0.9574362411282279\n",
      "Epoch: 13           Costo: 1.0237107291004868\n",
      "Epoch: 14           Costo: 1.0763780826872054\n",
      "Epoch: 15           Costo: 1.0426527708226982\n",
      "Epoch: 16           Costo: 1.2230848617987193\n",
      "Epoch: 17           Costo: 1.1402769866856661\n",
      "Epoch: 18           Costo: 1.3711064869707281\n",
      "Epoch: 19           Costo: 1.4034507898850876\n",
      "Epoch: 20           Costo: 1.405176920999181\n",
      "Epoch: 21           Costo: 1.4551691202683874\n",
      "Epoch: 22           Costo: 1.6739765942096707\n",
      "Epoch: 23           Costo: 1.7652670593695208\n",
      "Epoch: 24           Costo: 1.7913034105300893\n",
      "Epoch: 25           Costo: 1.7882943042841803\n",
      "Epoch: 26           Costo: 1.9957522236217156\n",
      "Epoch: 27           Costo: 1.965708218704569\n",
      "Epoch: 28           Costo: 1.9178789890896202\n",
      "Epoch: 29           Costo: 1.9306112326275207\n",
      "Epoch: 30           Costo: 1.8844940749081711\n",
      "Epoch: 31           Costo: 1.8725050705129447\n",
      "Epoch: 32           Costo: 1.8826990216428583\n",
      "Epoch: 33           Costo: 1.9215738528425028\n",
      "Epoch: 34           Costo: 1.9015824952992537\n",
      "Epoch: 35           Costo: 1.8676610710404125\n",
      "Epoch: 36           Costo: 1.8858888004042902\n",
      "Epoch: 37           Costo: 1.8581145984476253\n",
      "Epoch: 38           Costo: 1.8638325840776624\n",
      "Epoch: 39           Costo: 1.8573479049856016\n",
      "Epoch: 40           Costo: 1.8461987018585195\n",
      "Epoch: 41           Costo: 1.8761109113693235\n",
      "Epoch: 42           Costo: 1.8396251531080754\n",
      "Epoch: 43           Costo: 1.8538776089928368\n",
      "Epoch: 44           Costo: 1.8709181876616028\n",
      "Epoch: 45           Costo: 1.8315609678355138\n",
      "Epoch: 46           Costo: 1.858321392969652\n",
      "Epoch: 47           Costo: 2.0527874530445436\n",
      "Epoch: 48           Costo: 1.8857323243401272\n",
      "Epoch: 49           Costo: 1.8653839406100197\n",
      "Epoch: 50           Costo: 1.8644832277297976\n",
      "Epoch: 51           Costo: 1.8499707169966264\n",
      "Epoch: 52           Costo: 1.8437591674111122\n",
      "Epoch: 53           Costo: 1.8451385218446912\n",
      "Epoch: 54           Costo: 1.850576059818267\n",
      "Epoch: 55           Costo: 1.867305647459896\n",
      "Epoch: 56           Costo: 1.8785463825139146\n",
      "Epoch: 57           Costo: 1.8558453817801013\n",
      "Epoch: 58           Costo: 1.8680974346941197\n",
      "Epoch: 59           Costo: 1.8740347543629725\n",
      "Epoch: 60           Costo: 1.8665220076387563\n",
      "Epoch: 61           Costo: 1.8535427152026789\n",
      "Epoch: 62           Costo: 1.8753423105586657\n",
      "Epoch: 63           Costo: 1.8619711479273717\n",
      "Epoch: 64           Costo: 1.8526956855167032\n",
      "Epoch: 65           Costo: 1.8479973121122883\n",
      "Epoch: 66           Costo: 1.8569566304033467\n",
      "Epoch: 67           Costo: 1.842852593551981\n",
      "Epoch: 68           Costo: 1.8471731903336253\n",
      "Epoch: 69           Costo: 1.856244595701044\n",
      "Epoch: 70           Costo: 1.8302766463973301\n",
      "Epoch: 71           Costo: 1.8227463531494144\n",
      "Epoch: 72           Costo: 1.8347654041376988\n",
      "Epoch: 73           Costo: 1.8435550798069376\n",
      "Epoch: 74           Costo: 1.8356380919976676\n",
      "Epoch: 75           Costo: 1.8623018364472828\n",
      "Epoch: 76           Costo: 1.8412413061748858\n",
      "Epoch: 77           Costo: 1.8456747154756032\n",
      "Epoch: 78           Costo: 1.8352507324652239\n",
      "Epoch: 79           Costo: 1.8280935913866225\n",
      "Epoch: 80           Costo: 1.8317864539406514\n",
      "Epoch: 81           Costo: 1.821900796456771\n",
      "Epoch: 82           Costo: 1.8246695574847116\n",
      "Epoch: 83           Costo: 1.8346220980991006\n",
      "Epoch: 84           Costo: 1.8586308308081196\n",
      "Epoch: 85           Costo: 1.8300952538576982\n",
      "Epoch: 86           Costo: 1.8390087413787846\n",
      "Epoch: 87           Costo: 1.8326940590685077\n",
      "Epoch: 88           Costo: 1.8258973728526735\n",
      "Epoch: 89           Costo: 1.819834794347937\n",
      "Epoch: 90           Costo: 1.8258689358017655\n",
      "Epoch: 91           Costo: 1.8329541624676113\n",
      "Epoch: 92           Costo: 1.8351813652298663\n",
      "Epoch: 93           Costo: 1.840685592347926\n",
      "Epoch: 94           Costo: 1.8389422444863746\n",
      "Epoch: 95           Costo: 1.8359522245147004\n",
      "Epoch: 96           Costo: 1.8353725936196077\n",
      "Epoch: 97           Costo: 1.8340014405684053\n",
      "Epoch: 98           Costo: 1.833296322605827\n",
      "Epoch: 99           Costo: 1.8363044489513762\n",
      "Epoch: 100          Costo: 1.8359863760254609\n",
      "Epoch: 101          Costo: 1.8360573766448287\n",
      "Epoch: 102          Costo: 1.8350731494209982\n",
      "Epoch: 103          Costo: 1.8361766680804161\n",
      "Epoch: 104          Costo: 1.835944323539735\n",
      "Epoch: 105          Costo: 1.833995493108577\n",
      "Epoch: 106          Costo: 1.8359659962220598\n",
      "Epoch: 107          Costo: 1.835694375038147\n",
      "Epoch: 108          Costo: 1.8372401120445963\n",
      "Epoch: 109          Costo: 1.8353199960968725\n",
      "Epoch: 110          Costo: 1.836736503297631\n",
      "Epoch: 111          Costo: 1.8347669304500906\n",
      "Epoch: 112          Costo: 1.8361941987817936\n",
      "Epoch: 113          Costo: 1.8345619533278719\n",
      "Epoch: 114          Costo: 1.8352661395072916\n",
      "Epoch: 115          Costo: 1.835842719728297\n",
      "Epoch: 116          Costo: 1.8371471177447927\n",
      "Epoch: 117          Costo: 1.8364010292833526\n",
      "Epoch: 118          Costo: 1.834644623669711\n",
      "Epoch: 119          Costo: 1.8372661516883155\n",
      "Epoch: 120          Costo: 1.83610041444952\n",
      "Epoch: 121          Costo: 1.8368859958648691\n",
      "Epoch: 122          Costo: 1.8357873949137595\n",
      "Epoch: 123          Costo: 1.8360698348825617\n",
      "Epoch: 124          Costo: 1.8371638239513743\n",
      "Epoch: 125          Costo: 1.835673634572461\n",
      "Epoch: 126          Costo: 1.8362095340815456\n",
      "Epoch: 127          Costo: 1.8380520120534019\n",
      "Epoch: 128          Costo: 1.8371329641342171\n",
      "Epoch: 129          Costo: 1.8355417392470623\n",
      "Epoch: 130          Costo: 1.8348559331893934\n",
      "Epoch: 131          Costo: 1.836145909049294\n",
      "Epoch: 132          Costo: 1.834902049844914\n",
      "Epoch: 133          Costo: 1.8346906776861702\n",
      "Epoch: 134          Costo: 1.8354134546626701\n",
      "Epoch: 135          Costo: 1.8377975080230013\n",
      "Epoch: 136          Costo: 1.8351273874803018\n",
      "Epoch: 137          Costo: 1.8364830582792107\n",
      "Epoch: 138          Costo: 1.8355978439070941\n",
      "Epoch: 139          Costo: 1.8358137681267468\n",
      "Epoch: 140          Costo: 1.8361086258021266\n",
      "Epoch: 141          Costo: 1.834983105659485\n",
      "Epoch: 142          Costo: 1.8377509947256603\n",
      "Epoch: 143          Costo: 1.8336167415705564\n",
      "Epoch: 144          Costo: 1.8359369102391319\n",
      "Epoch: 145          Costo: 1.837305323427373\n",
      "Epoch: 146          Costo: 1.8360181140899663\n",
      "Epoch: 147          Costo: 1.834605968215246\n",
      "Epoch: 148          Costo: 1.835786660367793\n",
      "Epoch: 149          Costo: 1.8362766634334216\n",
      "Epoch: 150          Costo: 1.836140644333579\n",
      "Epoch: 151          Costo: 1.835002828944814\n",
      "Epoch: 152          Costo: 1.8350414609909058\n",
      "Epoch: 153          Costo: 1.8361257860877305\n",
      "Epoch: 154          Costo: 1.834384981068698\n",
      "Epoch: 155          Costo: 1.83647196444598\n",
      "Epoch: 156          Costo: 1.8347215444391438\n",
      "Epoch: 157          Costo: 1.8371926650134\n",
      "Epoch: 158          Costo: 1.8350101856751877\n",
      "Epoch: 159          Costo: 1.8338122222640303\n",
      "Epoch: 160          Costo: 1.836756792935459\n",
      "Epoch: 161          Costo: 1.8361421585083015\n",
      "Epoch: 162          Costo: 1.8351399315487262\n",
      "Epoch: 163          Costo: 1.8378368668122727\n",
      "Epoch: 164          Costo: 1.8355180699175049\n",
      "Epoch: 165          Costo: 1.8357453001629223\n",
      "Epoch: 166          Costo: 1.8357055768099693\n",
      "Epoch: 167          Costo: 1.8358238885619416\n",
      "Epoch: 168          Costo: 1.8360181819308905\n",
      "Epoch: 169          Costo: 1.8372782416777191\n",
      "Epoch: 170          Costo: 1.8362227745489639\n",
      "Epoch: 171          Costo: 1.835092499472879\n",
      "Epoch: 172          Costo: 1.8361769147352753\n",
      "Epoch: 173          Costo: 1.8347803510319083\n",
      "Epoch: 174          Costo: 1.8350584153695544\n",
      "Se acabaron los epochs, saliendo de la sesión de tensorflow.\n",
      "INFO:tensorflow:Restoring parameters from rnn2.ckpt\n",
      "Precisión: 0.1923000067472458\n"
     ]
    }
   ],
   "source": [
    "#Aumentando la taza de aprendizaje\n",
    "alfa = 0.1\n",
    "optimizador = tf.train.AdamOptimizer(learning_rate=alfa)\n",
    "paso_entrenamiento = optimizador.minimize(costo)\n",
    "\n",
    "archivo_modelo = \"rnn2.ckpt\"\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "numero_epochs = 175\n",
    "tamano_minibatch = 100\n",
    "display_step = 1\n",
    "\n",
    "# Muy importante la primera vez que se ejecuta inicializar todas las variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# La manera correcta de iniciar una sesión y realizar calculos\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Ciclos de entrenamiento\n",
    "    for epoch in range(numero_epochs):\n",
    "\n",
    "        #  Inicializa el costo promedio de todos los minibatches en 0\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        #  Calcula el número de minibatches que se pueden usar \n",
    "        total_batch = int(mnist.train.num_examples/tamano_minibatch)\n",
    "\n",
    "        #  Por cada minibatch\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            #  Utiliza un generador incluido en mnist que obtiene \n",
    "            #  tamano_minibatch ejemplos selecionados aleatoriamente del total\n",
    "            batch_x, batch_y = mnist.train.next_batch(tamano_minibatch)\n",
    "            \n",
    "            #  Ejecuta la ops del paso_entrenamiento para aprender \n",
    "            #  y la del costo, con el fin de mostrar el aprendizaje\n",
    "            _, c = sess.run([paso_entrenamiento, costo], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            #  Calcula el costo del minibatch y lo agrega al costo total\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        # Muestra los resultados\n",
    "        if epoch % display_step == 0:\n",
    "            print ((\"Epoch: \" + str(epoch)).ljust(20)\n",
    "                   + (\"Costo: \" + str(avg_cost)))\n",
    "    \n",
    "    #  Guarda la sesión en el archivo rnn2.cptk\n",
    "    saver.save(sess, archivo_modelo)\n",
    "    \n",
    "    print(\"Se acabaron los epochs, saliendo de la sesión de tensorflow.\")\n",
    "\n",
    "prediction_correcta = tf.equal(tf.argmax(estimado, 1), tf.argmax(y, 1))\n",
    "\n",
    "precision = tf.reduce_mean(tf.cast(prediction_correcta, \"float\"))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, archivo_modelo)\n",
    "    porcentaje_acierto = sess.run(precision, feed_dict={x: mnist.test.images,\n",
    "                                                        y: mnist.test.labels})\n",
    "    print(\"Precisión: {}\".format(porcentaje_acierto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0            Costo: 254.21471038818396\n",
      "Epoch: 1            Costo: 62.56794752120975\n",
      "Epoch: 2            Costo: 41.21759744990956\n",
      "Epoch: 3            Costo: 30.350189034071814\n",
      "Epoch: 4            Costo: 23.468277419697124\n",
      "Epoch: 5            Costo: 18.791855746108034\n",
      "Epoch: 6            Costo: 15.0309694515846\n",
      "Epoch: 7            Costo: 12.2764116409421\n",
      "Epoch: 8            Costo: 9.971785438114955\n",
      "Epoch: 9            Costo: 8.19853761590441\n",
      "Epoch: 10           Costo: 6.714460858339727\n",
      "Epoch: 11           Costo: 5.564960572212129\n",
      "Epoch: 12           Costo: 4.523332721494287\n",
      "Epoch: 13           Costo: 3.646768335725474\n",
      "Epoch: 14           Costo: 2.954550225110011\n",
      "Epoch: 15           Costo: 2.425433627400534\n",
      "Epoch: 16           Costo: 1.898528239601207\n",
      "Epoch: 17           Costo: 1.557473380023218\n",
      "Epoch: 18           Costo: 1.2693301802143944\n",
      "Epoch: 19           Costo: 1.0002131830044076\n",
      "Epoch: 20           Costo: 0.7379625281962043\n",
      "Epoch: 21           Costo: 0.6028432024959061\n",
      "Epoch: 22           Costo: 0.4608599397122349\n",
      "Epoch: 23           Costo: 0.42703172791977795\n",
      "Epoch: 24           Costo: 0.31741300304299164\n",
      "Epoch: 25           Costo: 0.27412294025464184\n",
      "Epoch: 26           Costo: 0.23193645545739927\n",
      "Epoch: 27           Costo: 0.18707353565871887\n",
      "Epoch: 28           Costo: 0.18900137031399014\n",
      "Epoch: 29           Costo: 0.15062598001471467\n",
      "Epoch: 30           Costo: 0.17502200746684726\n",
      "Epoch: 31           Costo: 0.14327403164671088\n",
      "Epoch: 32           Costo: 0.13973459246303174\n",
      "Epoch: 33           Costo: 0.12863868789764554\n",
      "Epoch: 34           Costo: 0.1271185609344428\n",
      "Epoch: 35           Costo: 0.11972790957120207\n",
      "Epoch: 36           Costo: 0.10196191161403914\n",
      "Epoch: 37           Costo: 0.1279391056259048\n",
      "Epoch: 38           Costo: 0.09448419448269492\n",
      "Epoch: 39           Costo: 0.12185751223405021\n",
      "Epoch: 40           Costo: 0.11753997118229803\n",
      "Epoch: 41           Costo: 0.07985296438848712\n",
      "Epoch: 42           Costo: 0.07826127295889648\n",
      "Epoch: 43           Costo: 0.06425819070582006\n",
      "Epoch: 44           Costo: 0.09050304033769312\n",
      "Epoch: 45           Costo: 0.08504754995462252\n",
      "Epoch: 46           Costo: 0.11081470013368305\n",
      "Epoch: 47           Costo: 0.08190466785185911\n",
      "Epoch: 48           Costo: 0.11412817499878491\n",
      "Epoch: 49           Costo: 0.07288339816681677\n",
      "Epoch: 50           Costo: 0.0954763889142608\n",
      "Epoch: 51           Costo: 0.05753126772823835\n",
      "Epoch: 52           Costo: 0.09706083494848886\n",
      "Epoch: 53           Costo: 0.07714958643221205\n",
      "Epoch: 54           Costo: 0.09706107618210139\n",
      "Epoch: 55           Costo: 0.062138709623177135\n",
      "Epoch: 56           Costo: 0.06611345926231402\n",
      "Epoch: 57           Costo: 0.07585801709452492\n",
      "Epoch: 58           Costo: 0.08480453641358238\n",
      "Epoch: 59           Costo: 0.06640255405100229\n",
      "Epoch: 60           Costo: 0.05901636605466745\n",
      "Epoch: 61           Costo: 0.04566597357390677\n",
      "Epoch: 62           Costo: 0.047914801028963394\n",
      "Epoch: 63           Costo: 0.08240055077179427\n",
      "Epoch: 64           Costo: 0.04767990537420437\n",
      "Epoch: 65           Costo: 0.06632077424690777\n",
      "Epoch: 66           Costo: 0.06625406664591611\n",
      "Epoch: 67           Costo: 0.06922102590320775\n",
      "Epoch: 68           Costo: 0.060214997410524544\n",
      "Epoch: 69           Costo: 0.056282167582601655\n",
      "Epoch: 70           Costo: 0.043630148518343\n",
      "Epoch: 71           Costo: 0.0715523966950851\n",
      "Epoch: 72           Costo: 0.0681360537738482\n",
      "Epoch: 73           Costo: 0.05335772963974027\n",
      "Epoch: 74           Costo: 0.057886359200991826\n",
      "Epoch: 75           Costo: 0.07626733909491575\n",
      "Epoch: 76           Costo: 0.06063243600615313\n",
      "Epoch: 77           Costo: 0.04791215295459763\n",
      "Epoch: 78           Costo: 0.039091648223751954\n",
      "Epoch: 79           Costo: 0.0505251999706063\n",
      "Epoch: 80           Costo: 0.07516989421653437\n",
      "Epoch: 81           Costo: 0.0286734028947948\n",
      "Epoch: 82           Costo: 0.04187376555070406\n",
      "Epoch: 83           Costo: 0.06022047105654447\n",
      "Epoch: 84           Costo: 0.04801627097338941\n",
      "Epoch: 85           Costo: 0.04073047017385945\n",
      "Epoch: 86           Costo: 0.054628935734820146\n",
      "Epoch: 87           Costo: 0.04472029314701886\n",
      "Epoch: 88           Costo: 0.06585317119486216\n",
      "Epoch: 89           Costo: 0.05024021558292905\n",
      "Epoch: 90           Costo: 0.043057801064752585\n",
      "Epoch: 91           Costo: 0.02732817194179797\n",
      "Epoch: 92           Costo: 0.04707765487304719\n",
      "Epoch: 93           Costo: 0.050434597705144656\n",
      "Epoch: 94           Costo: 0.06048920642979089\n",
      "Epoch: 95           Costo: 0.038695147922038806\n",
      "Epoch: 96           Costo: 0.030133782180200363\n",
      "Epoch: 97           Costo: 0.043056521119966024\n",
      "Epoch: 98           Costo: 0.03564618331670074\n",
      "Epoch: 99           Costo: 0.06302089614919743\n",
      "Epoch: 100          Costo: 0.04644423915653924\n",
      "Epoch: 101          Costo: 0.02905011490076224\n",
      "Epoch: 102          Costo: 0.02842797377399971\n",
      "Epoch: 103          Costo: 0.05155149464232738\n",
      "Epoch: 104          Costo: 0.04509907446913021\n",
      "Epoch: 105          Costo: 0.04050176262772554\n",
      "Epoch: 106          Costo: 0.06894387763869844\n",
      "Epoch: 107          Costo: 0.034946003339773705\n",
      "Epoch: 108          Costo: 0.03131039152396243\n",
      "Epoch: 109          Costo: 0.029497908164800534\n",
      "Epoch: 110          Costo: 0.026243908217255845\n",
      "Epoch: 111          Costo: 0.05040486701699468\n",
      "Epoch: 112          Costo: 0.05761649554341562\n",
      "Epoch: 113          Costo: 0.024117057867728876\n",
      "Epoch: 114          Costo: 0.021247271533666044\n",
      "Epoch: 115          Costo: 0.04925722276310143\n",
      "Epoch: 116          Costo: 0.031222832847290924\n",
      "Epoch: 117          Costo: 0.04583381489112837\n",
      "Epoch: 118          Costo: 0.034905614168082806\n",
      "Epoch: 119          Costo: 0.018048271702867207\n",
      "Epoch: 120          Costo: 0.06360315420545544\n",
      "Epoch: 121          Costo: 0.033075953702518805\n",
      "Epoch: 122          Costo: 0.048744750084169966\n",
      "Epoch: 123          Costo: 0.0407600509910362\n",
      "Epoch: 124          Costo: 0.04789102717500584\n",
      "Epoch: 125          Costo: 0.03812470897419419\n",
      "Epoch: 126          Costo: 0.039515883838014125\n",
      "Epoch: 127          Costo: 0.04881435896313693\n",
      "Epoch: 128          Costo: 0.029865929813818845\n",
      "Epoch: 129          Costo: 0.034484170018774726\n",
      "Epoch: 130          Costo: 0.05082744008664354\n",
      "Epoch: 131          Costo: 0.04601244015486347\n",
      "Epoch: 132          Costo: 0.019981412809405522\n",
      "Epoch: 133          Costo: 0.03173012953189319\n",
      "Epoch: 134          Costo: 0.02625086042128317\n",
      "Epoch: 135          Costo: 0.030459288891551143\n",
      "Epoch: 136          Costo: 0.039864012786315385\n",
      "Epoch: 137          Costo: 0.03233893295976739\n",
      "Epoch: 138          Costo: 0.04354948594239336\n",
      "Epoch: 139          Costo: 0.030988671162135772\n",
      "Epoch: 140          Costo: 0.025472193107510616\n",
      "Epoch: 141          Costo: 0.02775095504671219\n",
      "Epoch: 142          Costo: 0.03900436919947408\n",
      "Epoch: 143          Costo: 0.029007377014614144\n",
      "Epoch: 144          Costo: 0.02595191557354698\n",
      "Epoch: 145          Costo: 0.028195826413072493\n",
      "Epoch: 146          Costo: 0.052507573469604614\n",
      "Epoch: 147          Costo: 0.030141063029900893\n",
      "Epoch: 148          Costo: 0.03139865755135313\n",
      "Epoch: 149          Costo: 0.021426734058671015\n",
      "Epoch: 150          Costo: 0.012394162466655702\n",
      "Epoch: 151          Costo: 0.05476667682077577\n",
      "Epoch: 152          Costo: 0.054947833428431805\n",
      "Epoch: 153          Costo: 0.033626778211023\n",
      "Epoch: 154          Costo: 0.01863879228895044\n",
      "Epoch: 155          Costo: 0.02527878770245779\n",
      "Epoch: 156          Costo: 0.031399832253730796\n",
      "Epoch: 157          Costo: 0.024103115537615124\n",
      "Epoch: 158          Costo: 0.030910954458548665\n",
      "Epoch: 159          Costo: 0.0449634150984915\n",
      "Epoch: 160          Costo: 0.04448582852845129\n",
      "Epoch: 161          Costo: 0.02201402064570052\n",
      "Epoch: 162          Costo: 0.024170739082975776\n",
      "Epoch: 163          Costo: 0.020927438967268907\n",
      "Epoch: 164          Costo: 0.0352022656719715\n",
      "Epoch: 165          Costo: 0.04234119970755878\n",
      "Epoch: 166          Costo: 0.033182398978024656\n",
      "Epoch: 167          Costo: 0.022734971983686205\n",
      "Epoch: 168          Costo: 0.013973761179994616\n",
      "Epoch: 169          Costo: 0.022461959801003954\n",
      "Epoch: 170          Costo: 0.04418633865358347\n",
      "Epoch: 171          Costo: 0.028007662851219334\n",
      "Epoch: 172          Costo: 0.038188319482314485\n",
      "Epoch: 173          Costo: 0.0395093338031651\n",
      "Epoch: 174          Costo: 0.03339541947806375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se acabaron los epochs, saliendo de la sesión de tensorflow.\n",
      "INFO:tensorflow:Restoring parameters from rnn2.ckpt\n",
      "Precisión: 0.9628000259399414\n"
     ]
    }
   ],
   "source": [
    "#Reduciendo la taza de aprendizaje\n",
    "alfa = 0.0005\n",
    "optimizador = tf.train.AdamOptimizer(learning_rate=alfa)\n",
    "paso_entrenamiento = optimizador.minimize(costo)\n",
    "\n",
    "archivo_modelo = \"rnn2.ckpt\"\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "numero_epochs = 175\n",
    "tamano_minibatch = 100\n",
    "display_step = 1\n",
    "\n",
    "# Muy importante la primera vez que se ejecuta inicializar todas las variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# La manera correcta de iniciar una sesión y realizar calculos\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Ciclos de entrenamiento\n",
    "    for epoch in range(numero_epochs):\n",
    "\n",
    "        #  Inicializa el costo promedio de todos los minibatches en 0\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        #  Calcula el número de minibatches que se pueden usar \n",
    "        total_batch = int(mnist.train.num_examples/tamano_minibatch)\n",
    "\n",
    "        #  Por cada minibatch\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            #  Utiliza un generador incluido en mnist que obtiene \n",
    "            #  tamano_minibatch ejemplos selecionados aleatoriamente del total\n",
    "            batch_x, batch_y = mnist.train.next_batch(tamano_minibatch)\n",
    "            \n",
    "            #  Ejecuta la ops del paso_entrenamiento para aprender \n",
    "            #  y la del costo, con el fin de mostrar el aprendizaje\n",
    "            _, c = sess.run([paso_entrenamiento, costo], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            #  Calcula el costo del minibatch y lo agrega al costo total\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        # Muestra los resultados\n",
    "        if epoch % display_step == 0:\n",
    "            print ((\"Epoch: \" + str(epoch)).ljust(20)\n",
    "                   + (\"Costo: \" + str(avg_cost)))\n",
    "    \n",
    "    #  Guarda la sesión en el archivo rnn2.cptk\n",
    "    saver.save(sess, archivo_modelo)\n",
    "    \n",
    "    print(\"Se acabaron los epochs, saliendo de la sesión de tensorflow.\")\n",
    "\n",
    "prediction_correcta = tf.equal(tf.argmax(estimado, 1), tf.argmax(y, 1))\n",
    "\n",
    "precision = tf.reduce_mean(tf.cast(prediction_correcta, \"float\"))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, archivo_modelo)\n",
    "    porcentaje_acierto = sess.run(precision, feed_dict={x: mnist.test.images,\n",
    "                                                        y: mnist.test.labels})\n",
    "    print(\"Precisión: {}\".format(porcentaje_acierto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0            Costo: 280.005361230156\n",
      "Epoch: 1            Costo: 2.023594398715279\n",
      "Epoch: 2            Costo: 1.8592671244794656\n",
      "Epoch: 3            Costo: 1.7169317117604348\n",
      "Epoch: 4            Costo: 1.5895839383385393\n",
      "Epoch: 5            Costo: 1.5300214145400308\n",
      "Epoch: 6            Costo: 1.4773481858860358\n",
      "Epoch: 7            Costo: 1.4348641239513054\n",
      "Epoch: 8            Costo: 1.3779966463825921\n",
      "Epoch: 9            Costo: 1.3510505399920723\n",
      "Epoch: 10           Costo: 1.3020347636396232\n",
      "Epoch: 11           Costo: 1.2733697082779616\n",
      "Epoch: 12           Costo: 1.2043682751872313\n",
      "Epoch: 13           Costo: 1.17810735247352\n",
      "Epoch: 14           Costo: 1.127011257626793\n",
      "Epoch: 15           Costo: 1.0957499301433564\n",
      "Epoch: 16           Costo: 1.0646249361471694\n",
      "Epoch: 17           Costo: 1.057452653429726\n",
      "Epoch: 18           Costo: 0.9956239119443014\n",
      "Epoch: 19           Costo: 0.9840698994289749\n",
      "Epoch: 20           Costo: 0.9605577254295347\n",
      "Epoch: 21           Costo: 0.9636677595702077\n",
      "Epoch: 22           Costo: 0.9255119098316554\n",
      "Epoch: 23           Costo: 0.9036090841076584\n",
      "Epoch: 24           Costo: 0.8861392344128006\n",
      "Epoch: 25           Costo: 0.8827331026033925\n",
      "Epoch: 26           Costo: 0.8533173096721832\n",
      "Epoch: 27           Costo: 0.8523569820143956\n",
      "Epoch: 28           Costo: 0.838005539070476\n",
      "Epoch: 29           Costo: 0.8341037854281335\n",
      "Epoch: 30           Costo: 0.8239737030592833\n",
      "Epoch: 31           Costo: 0.8017454555901616\n",
      "Epoch: 32           Costo: 0.7906238844719791\n",
      "Epoch: 33           Costo: 0.7836629731004887\n",
      "Epoch: 34           Costo: 0.7661484830487855\n",
      "Epoch: 35           Costo: 0.7620772419734438\n",
      "Epoch: 36           Costo: 0.744288690035993\n",
      "Epoch: 37           Costo: 0.7467778566750608\n",
      "Epoch: 38           Costo: 0.7269186152111402\n",
      "Epoch: 39           Costo: 0.7234762671860775\n",
      "Epoch: 40           Costo: 0.7139521509408948\n",
      "Epoch: 41           Costo: 0.7063230775703084\n",
      "Epoch: 42           Costo: 0.7060707818378107\n",
      "Epoch: 43           Costo: 0.6865017118779104\n",
      "Epoch: 44           Costo: 0.674877308661288\n",
      "Epoch: 45           Costo: 0.6703814958442346\n",
      "Epoch: 46           Costo: 0.6660308490558093\n",
      "Epoch: 47           Costo: 0.6551727279749782\n",
      "Epoch: 48           Costo: 0.657468809160319\n",
      "Epoch: 49           Costo: 0.6393281903050165\n",
      "Epoch: 50           Costo: 0.6392012502930378\n",
      "Epoch: 51           Costo: 0.6375422941554675\n",
      "Epoch: 52           Costo: 0.6198973165858873\n",
      "Epoch: 53           Costo: 0.6234997780756518\n",
      "Epoch: 54           Costo: 0.6199417491934518\n",
      "Epoch: 55           Costo: 0.6108533158085562\n",
      "Epoch: 56           Costo: 0.5960392276265408\n",
      "Epoch: 57           Costo: 0.604810483238914\n",
      "Epoch: 58           Costo: 0.5924127114360984\n",
      "Epoch: 59           Costo: 0.5960055046731771\n",
      "Epoch: 60           Costo: 0.585854682651433\n",
      "Epoch: 61           Costo: 0.5813159189440984\n",
      "Epoch: 62           Costo: 0.5905056678436015\n",
      "Epoch: 63           Costo: 0.5720404982566836\n",
      "Epoch: 64           Costo: 0.5843083506280727\n",
      "Epoch: 65           Costo: 0.5708700904521075\n",
      "Epoch: 66           Costo: 0.5614110397208822\n",
      "Epoch: 67           Costo: 0.5626207296414814\n",
      "Epoch: 68           Costo: 0.5616899913549418\n",
      "Epoch: 69           Costo: 0.5670283255793832\n",
      "Epoch: 70           Costo: 0.5497894949804659\n",
      "Epoch: 71           Costo: 0.5513484631885182\n",
      "Epoch: 72           Costo: 0.5430188520930025\n",
      "Epoch: 73           Costo: 0.5465332637049933\n",
      "Epoch: 74           Costo: 0.539049908843907\n",
      "Epoch: 75           Costo: 0.5364152148365972\n",
      "Epoch: 76           Costo: 0.5395106211575594\n",
      "Epoch: 77           Costo: 0.537195730019699\n",
      "Epoch: 78           Costo: 0.5311681013757531\n",
      "Epoch: 79           Costo: 0.524652337973768\n",
      "Epoch: 80           Costo: 0.5328609734231778\n",
      "Epoch: 81           Costo: 0.5258607968417077\n",
      "Epoch: 82           Costo: 0.5234956951574843\n",
      "Epoch: 83           Costo: 0.5347142486680638\n",
      "Epoch: 84           Costo: 0.5368676888671792\n",
      "Epoch: 85           Costo: 0.5151649711077864\n",
      "Epoch: 86           Costo: 0.5160778924551873\n",
      "Epoch: 87           Costo: 0.5188923465121873\n",
      "Epoch: 88           Costo: 0.5116584105654198\n",
      "Epoch: 89           Costo: 0.5093724482980638\n",
      "Epoch: 90           Costo: 0.5063088057528842\n",
      "Epoch: 91           Costo: 0.5102768316052178\n",
      "Epoch: 92           Costo: 0.49988830766894604\n",
      "Epoch: 93           Costo: 0.5025205640901222\n",
      "Epoch: 94           Costo: 0.49976913563229836\n",
      "Epoch: 95           Costo: 0.4984695574099364\n",
      "Epoch: 96           Costo: 0.49710814879699217\n",
      "Epoch: 97           Costo: 0.49700901291587135\n",
      "Epoch: 98           Costo: 0.48943008794025955\n",
      "Epoch: 99           Costo: 0.4881359464471988\n",
      "Epoch: 100          Costo: 0.48645462916656024\n",
      "Epoch: 101          Costo: 0.4854448140751232\n",
      "Epoch: 102          Costo: 0.4856782134554598\n",
      "Epoch: 103          Costo: 0.47618954563682714\n",
      "Epoch: 104          Costo: 0.48010943854396976\n",
      "Epoch: 105          Costo: 0.47902501363645905\n",
      "Epoch: 106          Costo: 0.47186438110741696\n",
      "Epoch: 107          Costo: 0.47822303736751764\n",
      "Epoch: 108          Costo: 0.4747162745215676\n",
      "Epoch: 109          Costo: 0.4792837832461704\n",
      "Epoch: 110          Costo: 0.46846411073749694\n",
      "Epoch: 111          Costo: 0.464535452723503\n",
      "Epoch: 112          Costo: 0.4730209558660338\n",
      "Epoch: 113          Costo: 0.4642067704959352\n",
      "Epoch: 114          Costo: 0.4656596123901282\n",
      "Epoch: 115          Costo: 0.4662363645705311\n",
      "Epoch: 116          Costo: 0.4621082600680266\n",
      "Epoch: 117          Costo: 0.461804038394581\n",
      "Epoch: 118          Costo: 0.4590873471715238\n",
      "Epoch: 119          Costo: 0.45822135871106934\n",
      "Epoch: 120          Costo: 0.45696591873060577\n",
      "Epoch: 121          Costo: 0.460277567397464\n",
      "Epoch: 122          Costo: 0.4502049387043174\n",
      "Epoch: 123          Costo: 0.4564940436319876\n",
      "Epoch: 124          Costo: 0.4527831602367494\n",
      "Epoch: 125          Costo: 0.4501323334737257\n",
      "Epoch: 126          Costo: 0.4486651073802603\n",
      "Epoch: 127          Costo: 0.4457113088802857\n",
      "Epoch: 128          Costo: 0.4501865088126874\n",
      "Epoch: 129          Costo: 0.44805729646574377\n",
      "Epoch: 130          Costo: 0.4507302331111648\n",
      "Epoch: 131          Costo: 0.4420696211132134\n",
      "Epoch: 132          Costo: 0.4438999076864938\n",
      "Epoch: 133          Costo: 0.4370130484483458\n",
      "Epoch: 134          Costo: 0.44810380653901544\n",
      "Epoch: 135          Costo: 0.4406694264303553\n",
      "Epoch: 136          Costo: 0.4380161190845749\n",
      "Epoch: 137          Costo: 0.4376692348718646\n",
      "Epoch: 138          Costo: 0.4397034020857379\n",
      "Epoch: 139          Costo: 0.43599844444881775\n",
      "Epoch: 140          Costo: 0.43845235242084946\n",
      "Epoch: 141          Costo: 0.4345718264308841\n",
      "Epoch: 142          Costo: 0.4331604584780605\n",
      "Epoch: 143          Costo: 0.4308533317663452\n",
      "Epoch: 144          Costo: 0.4357310718297955\n",
      "Epoch: 145          Costo: 0.4302610736272553\n",
      "Epoch: 146          Costo: 0.4256309483538972\n",
      "Epoch: 147          Costo: 0.4337944009358235\n",
      "Epoch: 148          Costo: 0.42527563211592795\n",
      "Epoch: 149          Costo: 0.43233561521226754\n",
      "Epoch: 150          Costo: 0.4254666442491795\n",
      "Epoch: 151          Costo: 0.4250548402829603\n",
      "Epoch: 152          Costo: 0.42339374097910787\n",
      "Epoch: 153          Costo: 0.4246519134532322\n",
      "Epoch: 154          Costo: 0.4197827919504859\n",
      "Epoch: 155          Costo: 0.4194127324223523\n",
      "Epoch: 156          Costo: 0.4183513213829561\n",
      "Epoch: 157          Costo: 0.4241430376605555\n",
      "Epoch: 158          Costo: 0.4189762200550597\n",
      "Epoch: 159          Costo: 0.4159199973669922\n",
      "Epoch: 160          Costo: 0.40997575112364526\n",
      "Epoch: 161          Costo: 0.4182875867052512\n",
      "Epoch: 162          Costo: 0.4113678800788794\n",
      "Epoch: 163          Costo: 0.4108291058377783\n",
      "Epoch: 164          Costo: 0.4149636189775034\n",
      "Epoch: 165          Costo: 0.411356330405582\n",
      "Epoch: 166          Costo: 0.40451934256336974\n",
      "Epoch: 167          Costo: 0.41213443837382585\n",
      "Epoch: 168          Costo: 0.40284056793559686\n",
      "Epoch: 169          Costo: 0.4133530929413712\n",
      "Epoch: 170          Costo: 0.4076833216710522\n",
      "Epoch: 171          Costo: 0.4003244436599993\n",
      "Epoch: 172          Costo: 0.4051863548701459\n",
      "Epoch: 173          Costo: 0.40233063418756804\n",
      "Epoch: 174          Costo: 0.4003539644588123\n",
      "Se acabaron los epochs, saliendo de la sesión de tensorflow.\n",
      "INFO:tensorflow:Restoring parameters from rnn2.ckpt\n",
      "Precisión: 0.8655999898910522\n"
     ]
    }
   ],
   "source": [
    "#Método 1\n",
    "alfa = 0.6\n",
    "optimizador = tf.train.ProximalAdagradOptimizer(learning_rate=alfa)\n",
    "paso_entrenamiento = optimizador.minimize(costo)\n",
    "\n",
    "archivo_modelo = \"rnn2.ckpt\"\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "numero_epochs = 175\n",
    "tamano_minibatch = 100\n",
    "display_step = 1\n",
    "\n",
    "# Muy importante la primera vez que se ejecuta inicializar todas las variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# La manera correcta de iniciar una sesión y realizar calculos\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Ciclos de entrenamiento\n",
    "    for epoch in range(numero_epochs):\n",
    "\n",
    "        #  Inicializa el costo promedio de todos los minibatches en 0\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        #  Calcula el número de minibatches que se pueden usar \n",
    "        total_batch = int(mnist.train.num_examples/tamano_minibatch)\n",
    "\n",
    "        #  Por cada minibatch\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            #  Utiliza un generador incluido en mnist que obtiene \n",
    "            #  tamano_minibatch ejemplos selecionados aleatoriamente del total\n",
    "            batch_x, batch_y = mnist.train.next_batch(tamano_minibatch)\n",
    "            \n",
    "            #  Ejecuta la ops del paso_entrenamiento para aprender \n",
    "            #  y la del costo, con el fin de mostrar el aprendizaje\n",
    "            _, c = sess.run([paso_entrenamiento, costo], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            #  Calcula el costo del minibatch y lo agrega al costo total\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        # Muestra los resultados\n",
    "        if epoch % display_step == 0:\n",
    "            print ((\"Epoch: \" + str(epoch)).ljust(20)\n",
    "                   + (\"Costo: \" + str(avg_cost)))\n",
    "    \n",
    "    #  Guarda la sesión en el archivo rnn2.cptk\n",
    "    saver.save(sess, archivo_modelo)\n",
    "    \n",
    "    print(\"Se acabaron los epochs, saliendo de la sesión de tensorflow.\")\n",
    "\n",
    "prediction_correcta = tf.equal(tf.argmax(estimado, 1), tf.argmax(y, 1))\n",
    "\n",
    "precision = tf.reduce_mean(tf.cast(prediction_correcta, \"float\"))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, archivo_modelo)\n",
    "    porcentaje_acierto = sess.run(precision, feed_dict={x: mnist.test.images,\n",
    "                                                        y: mnist.test.labels})\n",
    "    print(\"Precisión: {}\".format(porcentaje_acierto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0            Costo: 370.94552099054516\n",
      "Epoch: 1            Costo: 104.45338495081121\n",
      "Epoch: 2            Costo: 71.81907029585412\n",
      "Epoch: 3            Costo: 55.93650263006032\n",
      "Epoch: 4            Costo: 46.732541037472835\n",
      "Epoch: 5            Costo: 40.10797598752108\n",
      "Epoch: 6            Costo: 35.9858842225508\n",
      "Epoch: 7            Costo: 30.73039533755996\n",
      "Epoch: 8            Costo: 28.954560588489883\n",
      "Epoch: 9            Costo: 25.61892104343932\n",
      "Epoch: 10           Costo: 24.023249946074053\n",
      "Epoch: 11           Costo: 21.414994392936887\n",
      "Epoch: 12           Costo: 19.696618300134492\n",
      "Epoch: 13           Costo: 18.199477016536342\n",
      "Epoch: 14           Costo: 17.29295752855865\n",
      "Epoch: 15           Costo: 16.036629317944705\n",
      "Epoch: 16           Costo: 14.973961072618293\n",
      "Epoch: 17           Costo: 14.06086731344461\n",
      "Epoch: 18           Costo: 12.834988652711568\n",
      "Epoch: 19           Costo: 12.525297011984165\n",
      "Epoch: 20           Costo: 11.485162926610268\n",
      "Epoch: 21           Costo: 11.088703446447923\n",
      "Epoch: 22           Costo: 9.936932106881322\n",
      "Epoch: 23           Costo: 9.715785504885993\n",
      "Epoch: 24           Costo: 9.191103437983173\n",
      "Epoch: 25           Costo: 8.660700907084601\n",
      "Epoch: 26           Costo: 8.13375004802502\n",
      "Epoch: 27           Costo: 7.818570070295669\n",
      "Epoch: 28           Costo: 7.2729474253892805\n",
      "Epoch: 29           Costo: 6.929601615322516\n",
      "Epoch: 30           Costo: 6.618323008044013\n",
      "Epoch: 31           Costo: 6.255986292565603\n",
      "Epoch: 32           Costo: 5.888827072384329\n",
      "Epoch: 33           Costo: 5.708040775547726\n",
      "Epoch: 34           Costo: 5.085820381996486\n",
      "Epoch: 35           Costo: 5.253492539479396\n",
      "Epoch: 36           Costo: 4.711880346440951\n",
      "Epoch: 37           Costo: 4.76154683062872\n",
      "Epoch: 38           Costo: 4.118295787508197\n",
      "Epoch: 39           Costo: 4.196792302074895\n",
      "Epoch: 40           Costo: 3.939812421387378\n",
      "Epoch: 41           Costo: 3.680298081119497\n",
      "Epoch: 42           Costo: 3.5773215064724666\n",
      "Epoch: 43           Costo: 3.2397010515620255\n",
      "Epoch: 44           Costo: 3.2525734343798383\n",
      "Epoch: 45           Costo: 2.9325804284362342\n",
      "Epoch: 46           Costo: 2.9967945746538938\n",
      "Epoch: 47           Costo: 2.7262693189005236\n",
      "Epoch: 48           Costo: 2.4311756110850795\n",
      "Epoch: 49           Costo: 2.538172094224041\n",
      "Epoch: 50           Costo: 2.338593687418217\n",
      "Epoch: 51           Costo: 2.2414333822484718\n",
      "Epoch: 52           Costo: 2.128218869142147\n",
      "Epoch: 53           Costo: 1.9317425938695874\n",
      "Epoch: 54           Costo: 1.785509025535672\n",
      "Epoch: 55           Costo: 1.852071911060999\n",
      "Epoch: 56           Costo: 1.7602372669235125\n",
      "Epoch: 57           Costo: 1.5864099678277466\n",
      "Epoch: 58           Costo: 1.5224442562441514\n",
      "Epoch: 59           Costo: 1.4060561398670397\n",
      "Epoch: 60           Costo: 1.3543372569736578\n",
      "Epoch: 61           Costo: 1.2856469673099686\n",
      "Epoch: 62           Costo: 1.222658843449293\n",
      "Epoch: 63           Costo: 1.1949244583323304\n",
      "Epoch: 64           Costo: 1.0463149730759322\n",
      "Epoch: 65           Costo: 1.0683085774520995\n",
      "Epoch: 66           Costo: 0.9707741567526591\n",
      "Epoch: 67           Costo: 0.9656535864365703\n",
      "Epoch: 68           Costo: 0.8609452013457981\n",
      "Epoch: 69           Costo: 0.8336679957786719\n",
      "Epoch: 70           Costo: 0.7803846849876487\n",
      "Epoch: 71           Costo: 0.7396667526448606\n",
      "Epoch: 72           Costo: 0.6846910288676473\n",
      "Epoch: 73           Costo: 0.6629645370203952\n",
      "Epoch: 74           Costo: 0.6173463837789251\n",
      "Epoch: 75           Costo: 0.6263718590131083\n",
      "Epoch: 76           Costo: 0.5343741791742969\n",
      "Epoch: 77           Costo: 0.5576941741701159\n",
      "Epoch: 78           Costo: 0.47486154295809097\n",
      "Epoch: 79           Costo: 0.4326836406312061\n",
      "Epoch: 80           Costo: 0.4720690451745048\n",
      "Epoch: 81           Costo: 0.40718930472407555\n",
      "Epoch: 82           Costo: 0.3770608195002428\n",
      "Epoch: 83           Costo: 0.3916276519663094\n",
      "Epoch: 84           Costo: 0.33228606774752145\n",
      "Epoch: 85           Costo: 0.3063599201757261\n",
      "Epoch: 86           Costo: 0.27095714437698104\n",
      "Epoch: 87           Costo: 0.31270245968574956\n",
      "Epoch: 88           Costo: 0.28938286864803797\n",
      "Epoch: 89           Costo: 0.24280063091231993\n",
      "Epoch: 90           Costo: 0.23819642749070344\n",
      "Epoch: 91           Costo: 0.17197119952936343\n",
      "Epoch: 92           Costo: 0.22061043975664357\n",
      "Epoch: 93           Costo: 0.20754305867886558\n",
      "Epoch: 94           Costo: 0.17487761624401463\n",
      "Epoch: 95           Costo: 0.14942222093139004\n",
      "Epoch: 96           Costo: 0.15755445380971295\n",
      "Epoch: 97           Costo: 0.13216597733724048\n",
      "Epoch: 98           Costo: 0.1436332247323004\n",
      "Epoch: 99           Costo: 0.12457843126945778\n",
      "Epoch: 100          Costo: 0.13066552055200759\n",
      "Epoch: 101          Costo: 0.09990422116388284\n",
      "Epoch: 102          Costo: 0.09186639339877918\n",
      "Epoch: 103          Costo: 0.11143672275527157\n",
      "Epoch: 104          Costo: 0.07796234874860146\n",
      "Epoch: 105          Costo: 0.07972691339643714\n",
      "Epoch: 106          Costo: 0.08562173752408009\n",
      "Epoch: 107          Costo: 0.07304480285960896\n",
      "Epoch: 108          Costo: 0.06524364613240911\n",
      "Epoch: 109          Costo: 0.06543601480511355\n",
      "Epoch: 110          Costo: 0.06016220482296076\n",
      "Epoch: 111          Costo: 0.05907225770212955\n",
      "Epoch: 112          Costo: 0.051374769829735864\n",
      "Epoch: 113          Costo: 0.05169703846759477\n",
      "Epoch: 114          Costo: 0.04529036246172341\n",
      "Epoch: 115          Costo: 0.03914821385962106\n",
      "Epoch: 116          Costo: 0.043166068682182576\n",
      "Epoch: 117          Costo: 0.02927330664531087\n",
      "Epoch: 118          Costo: 0.042488736246997914\n",
      "Epoch: 119          Costo: 0.025602290856989684\n",
      "Epoch: 120          Costo: 0.02747412064612344\n",
      "Epoch: 121          Costo: 0.029369471161054168\n",
      "Epoch: 122          Costo: 0.028255208942220444\n",
      "Epoch: 123          Costo: 0.02233700043127036\n",
      "Epoch: 124          Costo: 0.019101450549843555\n",
      "Epoch: 125          Costo: 0.02489073822239122\n",
      "Epoch: 126          Costo: 0.01683494410935178\n",
      "Epoch: 127          Costo: 0.018521931399098173\n",
      "Epoch: 128          Costo: 0.013773029760413879\n",
      "Epoch: 129          Costo: 0.013765255625220118\n",
      "Epoch: 130          Costo: 0.010240059363472238\n",
      "Epoch: 131          Costo: 0.009941766959258259\n",
      "Epoch: 132          Costo: 0.009970228659167355\n",
      "Epoch: 133          Costo: 0.008473978605599118\n",
      "Epoch: 134          Costo: 0.007669947500718036\n",
      "Epoch: 135          Costo: 0.004594972312562856\n",
      "Epoch: 136          Costo: 0.005431047857129317\n",
      "Epoch: 137          Costo: 0.004861586792598336\n",
      "Epoch: 138          Costo: 0.00570235524653995\n",
      "Epoch: 139          Costo: 0.0030280377852775\n",
      "Epoch: 140          Costo: 0.0022159589282869465\n",
      "Epoch: 141          Costo: 0.0023816847984847573\n",
      "Epoch: 142          Costo: 0.001184126708672228\n",
      "Epoch: 143          Costo: 0.001236938419152487\n",
      "Epoch: 144          Costo: 0.0008467976770085555\n",
      "Epoch: 145          Costo: 0.0012685309997102036\n",
      "Epoch: 146          Costo: 0.0015404897244175914\n",
      "Epoch: 147          Costo: 0.000585445065425982\n",
      "Epoch: 148          Costo: 0.0006308896914973743\n",
      "Epoch: 149          Costo: 0.00016540660935162506\n",
      "Epoch: 150          Costo: 0.0005753823234319355\n",
      "Epoch: 151          Costo: 9.157771280129135e-05\n",
      "Epoch: 152          Costo: 0.0005850375087647883\n",
      "Epoch: 153          Costo: 0.00022312867268297148\n",
      "Epoch: 154          Costo: 8.341851451407226e-05\n",
      "Epoch: 155          Costo: 0.0002771050772016817\n",
      "Epoch: 156          Costo: 1.1563513870357679e-07\n",
      "Epoch: 157          Costo: 2.4298435370843865e-08\n",
      "Epoch: 158          Costo: 1.0789461993742195e-08\n",
      "Epoch: 159          Costo: 6.6366863209084535e-09\n",
      "Epoch: 160          Costo: 7.369275732264798e-09\n",
      "Epoch: 161          Costo: 7.070176529889631e-09\n",
      "Epoch: 162          Costo: 6.181530076900608e-09\n",
      "Epoch: 163          Costo: 6.22921343799861e-09\n",
      "Epoch: 164          Costo: 5.505291416276555e-09\n",
      "Epoch: 165          Costo: 5.7111983025654405e-09\n",
      "Epoch: 166          Costo: 5.518297008435332e-09\n",
      "Epoch: 167          Costo: 4.831219586811448e-09\n",
      "Epoch: 168          Costo: 5.297219730738719e-09\n",
      "Epoch: 169          Costo: 4.311035514832763e-09\n",
      "Epoch: 170          Costo: 5.0783082680442e-09\n",
      "Epoch: 171          Costo: 4.790039105967927e-09\n",
      "Epoch: 172          Costo: 4.402068609430578e-09\n",
      "Epoch: 173          Costo: 4.159315504540213e-09\n",
      "Epoch: 174          Costo: 4.408570663275409e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se acabaron los epochs, saliendo de la sesión de tensorflow.\n",
      "INFO:tensorflow:Restoring parameters from rnn2.ckpt\n",
      "Precisión: 0.9336000084877014\n"
     ]
    }
   ],
   "source": [
    "#Método 2\n",
    "alfa = 0.6\n",
    "optimizador = tf.train.AdadeltaOptimizer(learning_rate=alfa)\n",
    "paso_entrenamiento = optimizador.minimize(costo)\n",
    "\n",
    "archivo_modelo = \"rnn2.ckpt\"\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "numero_epochs = 175\n",
    "tamano_minibatch = 100\n",
    "display_step = 1\n",
    "\n",
    "# Muy importante la primera vez que se ejecuta inicializar todas las variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# La manera correcta de iniciar una sesión y realizar calculos\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Ciclos de entrenamiento\n",
    "    for epoch in range(numero_epochs):\n",
    "\n",
    "        #  Inicializa el costo promedio de todos los minibatches en 0\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        #  Calcula el número de minibatches que se pueden usar \n",
    "        total_batch = int(mnist.train.num_examples/tamano_minibatch)\n",
    "\n",
    "        #  Por cada minibatch\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            #  Utiliza un generador incluido en mnist que obtiene \n",
    "            #  tamano_minibatch ejemplos selecionados aleatoriamente del total\n",
    "            batch_x, batch_y = mnist.train.next_batch(tamano_minibatch)\n",
    "            \n",
    "            #  Ejecuta la ops del paso_entrenamiento para aprender \n",
    "            #  y la del costo, con el fin de mostrar el aprendizaje\n",
    "            _, c = sess.run([paso_entrenamiento, costo], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            #  Calcula el costo del minibatch y lo agrega al costo total\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        # Muestra los resultados\n",
    "        if epoch % display_step == 0:\n",
    "            print ((\"Epoch: \" + str(epoch)).ljust(20)\n",
    "                   + (\"Costo: \" + str(avg_cost)))\n",
    "    \n",
    "    #  Guarda la sesión en el archivo rnn2.cptk\n",
    "    saver.save(sess, archivo_modelo)\n",
    "    \n",
    "    print(\"Se acabaron los epochs, saliendo de la sesión de tensorflow.\")\n",
    "\n",
    "prediction_correcta = tf.equal(tf.argmax(estimado, 1), tf.argmax(y, 1))\n",
    "\n",
    "precision = tf.reduce_mean(tf.cast(prediction_correcta, \"float\"))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, archivo_modelo)\n",
    "    porcentaje_acierto = sess.run(precision, feed_dict={x: mnist.test.images,\n",
    "                                                        y: mnist.test.labels})\n",
    "    print(\"Precisión: {}\".format(porcentaje_acierto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0            Costo: 100.5326173076838\n",
      "Epoch: 1            Costo: 24.293468637316053\n",
      "Epoch: 2            Costo: 13.89282890653465\n",
      "Epoch: 3            Costo: 9.051084436480894\n",
      "Epoch: 4            Costo: 5.875730512371206\n",
      "Epoch: 5            Costo: 3.9480447181074205\n",
      "Epoch: 6            Costo: 2.4665073531256128\n",
      "Epoch: 7            Costo: 2.2419377844511645\n",
      "Epoch: 8            Costo: 1.5858777416452772\n",
      "Epoch: 9            Costo: 1.2279949018589396\n",
      "Epoch: 10           Costo: 1.0745202171916584\n",
      "Epoch: 11           Costo: 0.8865324347168612\n",
      "Epoch: 12           Costo: 0.8450870968488302\n",
      "Epoch: 13           Costo: 0.697859825610904\n",
      "Epoch: 14           Costo: 0.7329831353162285\n",
      "Epoch: 15           Costo: 0.6770642213092567\n",
      "Epoch: 16           Costo: 0.6164504264123338\n",
      "Epoch: 17           Costo: 0.5308579060216483\n",
      "Epoch: 18           Costo: 0.5517203094027436\n",
      "Epoch: 19           Costo: 0.557607201105532\n",
      "Epoch: 20           Costo: 0.5494761097596081\n",
      "Epoch: 21           Costo: 0.444203520570091\n",
      "Epoch: 22           Costo: 0.4114898908149502\n",
      "Epoch: 23           Costo: 0.5107581603536906\n",
      "Epoch: 24           Costo: 0.3507340951978141\n",
      "Epoch: 25           Costo: 0.45348418892692544\n",
      "Epoch: 26           Costo: 0.38519184016264224\n",
      "Epoch: 27           Costo: 0.3445710486092425\n",
      "Epoch: 28           Costo: 0.412997566630526\n",
      "Epoch: 29           Costo: 0.29170453736620544\n",
      "Epoch: 30           Costo: 0.32952598291334767\n",
      "Epoch: 31           Costo: 0.3389263742234486\n",
      "Epoch: 32           Costo: 0.30552745917130963\n",
      "Epoch: 33           Costo: 0.282575097401362\n",
      "Epoch: 34           Costo: 0.2849564533224734\n",
      "Epoch: 35           Costo: 0.3660833427407513\n",
      "Epoch: 36           Costo: 0.24656572544237443\n",
      "Epoch: 37           Costo: 0.32497232152422173\n",
      "Epoch: 38           Costo: 0.2938803233301562\n",
      "Epoch: 39           Costo: 0.2604415986145288\n",
      "Epoch: 40           Costo: 0.2547114144967852\n",
      "Epoch: 41           Costo: 0.23841856345902596\n",
      "Epoch: 42           Costo: 0.2638827201330835\n",
      "Epoch: 43           Costo: 0.2862269864315837\n",
      "Epoch: 44           Costo: 0.2626497630077703\n",
      "Epoch: 45           Costo: 0.2513357012911376\n",
      "Epoch: 46           Costo: 0.26891986832249654\n",
      "Epoch: 47           Costo: 0.24426538751847726\n",
      "Epoch: 48           Costo: 0.17249206043923135\n",
      "Epoch: 49           Costo: 0.23684122768464716\n",
      "Epoch: 50           Costo: 0.2038964555107053\n",
      "Epoch: 51           Costo: 0.22554066113582563\n",
      "Epoch: 52           Costo: 0.23983349587510816\n",
      "Epoch: 53           Costo: 0.20009574253691936\n",
      "Epoch: 54           Costo: 0.2070930141313617\n",
      "Epoch: 55           Costo: 0.17133493209007636\n",
      "Epoch: 56           Costo: 0.17284719386565825\n",
      "Epoch: 57           Costo: 0.20156098346164125\n",
      "Epoch: 58           Costo: 0.24010878769287744\n",
      "Epoch: 59           Costo: 0.1843571272678367\n",
      "Epoch: 60           Costo: 0.1696980199755353\n",
      "Epoch: 61           Costo: 0.17318805270614954\n",
      "Epoch: 62           Costo: 0.20407635547462438\n",
      "Epoch: 63           Costo: 0.1528098100111367\n",
      "Epoch: 64           Costo: 0.20975010856328075\n",
      "Epoch: 65           Costo: 0.16058239857986545\n",
      "Epoch: 66           Costo: 0.17713776012131197\n",
      "Epoch: 67           Costo: 0.17653787914702296\n",
      "Epoch: 68           Costo: 0.17554943042021737\n",
      "Epoch: 69           Costo: 0.16789273446901112\n",
      "Epoch: 70           Costo: 0.24986509948867575\n",
      "Epoch: 71           Costo: 0.142380167963684\n",
      "Epoch: 72           Costo: 0.17180861314618726\n",
      "Epoch: 73           Costo: 0.15031656741024457\n",
      "Epoch: 74           Costo: 0.1536942835326236\n",
      "Epoch: 75           Costo: 0.18452956247848085\n",
      "Epoch: 76           Costo: 0.15147262265634362\n",
      "Epoch: 77           Costo: 0.1857243058618218\n",
      "Epoch: 78           Costo: 0.14991228069715953\n",
      "Epoch: 79           Costo: 0.147813509348625\n",
      "Epoch: 80           Costo: 0.1673107329686123\n",
      "Epoch: 81           Costo: 0.14701847170690138\n",
      "Epoch: 82           Costo: 0.13628621434620525\n",
      "Epoch: 83           Costo: 0.17340423227859247\n",
      "Epoch: 84           Costo: 0.16527340525071058\n",
      "Epoch: 85           Costo: 0.128007072572148\n",
      "Epoch: 86           Costo: 0.18114250358867032\n",
      "Epoch: 87           Costo: 0.13536073333846843\n",
      "Epoch: 88           Costo: 0.11200514348099513\n",
      "Epoch: 89           Costo: 0.19033287092970383\n",
      "Epoch: 90           Costo: 0.15003401737952154\n",
      "Epoch: 91           Costo: 0.1307443532442827\n",
      "Epoch: 92           Costo: 0.09110795603597759\n",
      "Epoch: 93           Costo: 0.16179705126637903\n",
      "Epoch: 94           Costo: 0.1487561539226189\n",
      "Epoch: 95           Costo: 0.13331078351497277\n",
      "Epoch: 96           Costo: 0.10750640687184831\n",
      "Epoch: 97           Costo: 0.1924298650725454\n",
      "Epoch: 98           Costo: 0.14428603685726452\n",
      "Epoch: 99           Costo: 0.1104942293119299\n",
      "Epoch: 100          Costo: 0.15129026066429113\n",
      "Epoch: 101          Costo: 0.1738618601460213\n",
      "Epoch: 102          Costo: 0.11331481254842911\n",
      "Epoch: 103          Costo: 0.08637987788496676\n",
      "Epoch: 104          Costo: 0.168578834308187\n",
      "Epoch: 105          Costo: 0.12889645961929003\n",
      "Epoch: 106          Costo: 0.11235235053638651\n",
      "Epoch: 107          Costo: 0.08108408161457356\n",
      "Epoch: 108          Costo: 0.12872726386844588\n",
      "Epoch: 109          Costo: 0.10423207619915138\n",
      "Epoch: 110          Costo: 0.10899119924010447\n",
      "Epoch: 111          Costo: 0.11679911252654983\n",
      "Epoch: 112          Costo: 0.08141509656244876\n",
      "Epoch: 113          Costo: 0.1546560860417932\n",
      "Epoch: 114          Costo: 0.17274655410247702\n",
      "Epoch: 115          Costo: 0.0745575280912278\n",
      "Epoch: 116          Costo: 0.13792680986493655\n",
      "Epoch: 117          Costo: 0.12813196630724538\n",
      "Epoch: 118          Costo: 0.10397950746751601\n",
      "Epoch: 119          Costo: 0.12789087749105923\n",
      "Epoch: 120          Costo: 0.09253069677315988\n",
      "Epoch: 121          Costo: 0.12459135423380928\n",
      "Epoch: 122          Costo: 0.10928559625451711\n",
      "Epoch: 123          Costo: 0.10829626265378334\n",
      "Epoch: 124          Costo: 0.1509328243713278\n",
      "Epoch: 125          Costo: 0.14024131734411877\n",
      "Epoch: 126          Costo: 0.12104037134391306\n",
      "Epoch: 127          Costo: 0.10225386203527026\n",
      "Epoch: 128          Costo: 0.12012151339126859\n",
      "Epoch: 129          Costo: 0.06147289026705694\n",
      "Epoch: 130          Costo: 0.11753657802188978\n",
      "Epoch: 131          Costo: 0.1133043662809365\n",
      "Epoch: 132          Costo: 0.07600108680483063\n",
      "Epoch: 133          Costo: 0.16898924999228873\n",
      "Epoch: 134          Costo: 0.11640854584289848\n",
      "Epoch: 135          Costo: 0.09578275352684788\n",
      "Epoch: 136          Costo: 0.14178932791505136\n",
      "Epoch: 137          Costo: 0.11606566736424755\n",
      "Epoch: 138          Costo: 0.13564439117883284\n",
      "Epoch: 139          Costo: 0.11680814663071745\n",
      "Epoch: 140          Costo: 0.07032506588530175\n",
      "Epoch: 141          Costo: 0.10894023546883544\n",
      "Epoch: 142          Costo: 0.07954666142354956\n",
      "Epoch: 143          Costo: 0.09713192994592228\n",
      "Epoch: 144          Costo: 0.09380253567626773\n",
      "Epoch: 145          Costo: 0.1412272256503776\n",
      "Epoch: 146          Costo: 0.1043392859808823\n",
      "Epoch: 147          Costo: 0.1393105386100668\n",
      "Epoch: 148          Costo: 0.07577193687781568\n",
      "Epoch: 149          Costo: 0.12218190422189082\n",
      "Epoch: 150          Costo: 0.10984112555909988\n",
      "Epoch: 151          Costo: 0.10401147436414725\n",
      "Epoch: 152          Costo: 0.07301137740088857\n",
      "Epoch: 153          Costo: 0.11732540580187101\n",
      "Epoch: 154          Costo: 0.07733124064721252\n",
      "Epoch: 155          Costo: 0.08443316277968582\n",
      "Epoch: 156          Costo: 0.11018841838986082\n",
      "Epoch: 157          Costo: 0.10891680498532774\n",
      "Epoch: 158          Costo: 0.07724313874038648\n",
      "Epoch: 159          Costo: 0.08537321068645598\n",
      "Epoch: 160          Costo: 0.10810033796533894\n",
      "Epoch: 161          Costo: 0.09078762035371768\n",
      "Epoch: 162          Costo: 0.08343928671138001\n",
      "Epoch: 163          Costo: 0.12061396442979203\n",
      "Epoch: 164          Costo: 0.09230028083456432\n",
      "Epoch: 165          Costo: 0.08076529024899491\n",
      "Epoch: 166          Costo: 0.07000724446017273\n",
      "Epoch: 167          Costo: 0.05716564877936552\n",
      "Epoch: 168          Costo: 0.08858979546539605\n",
      "Epoch: 169          Costo: 0.12042084094301143\n",
      "Epoch: 170          Costo: 0.07852615008802856\n",
      "Epoch: 171          Costo: 0.12717348395883638\n",
      "Epoch: 172          Costo: 0.07019783821977654\n",
      "Epoch: 173          Costo: 0.05769902143122204\n",
      "Epoch: 174          Costo: 0.06233209327883519\n",
      "Se acabaron los epochs, saliendo de la sesión de tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from rnn2.ckpt\n",
      "Precisión: 0.9761999845504761\n"
     ]
    }
   ],
   "source": [
    "#Cambiando el tama;o de los minichatch\n",
    "alfa = 0.001\n",
    "optimizador = tf.train.AdamOptimizer(learning_rate=alfa)\n",
    "paso_entrenamiento = optimizador.minimize(costo)\n",
    "\n",
    "archivo_modelo = \"rnn2.ckpt\"\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "numero_epochs = 175\n",
    "tamano_minibatch = 30\n",
    "\n",
    "display_step = 1\n",
    "\n",
    "# Muy importante la primera vez que se ejecuta inicializar todas las variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# La manera correcta de iniciar una sesión y realizar calculos\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Ciclos de entrenamiento\n",
    "    for epoch in range(numero_epochs):\n",
    "\n",
    "        #  Inicializa el costo promedio de todos los minibatches en 0\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        #  Calcula el número de minibatches que se pueden usar \n",
    "        total_batch = int(mnist.train.num_examples/tamano_minibatch)\n",
    "\n",
    "        #  Por cada minibatch\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            #  Utiliza un generador incluido en mnist que obtiene \n",
    "            #  tamano_minibatch ejemplos selecionados aleatoriamente del total\n",
    "            batch_x, batch_y = mnist.train.next_batch(tamano_minibatch)\n",
    "            \n",
    "            #  Ejecuta la ops del paso_entrenamiento para aprender \n",
    "            #  y la del costo, con el fin de mostrar el aprendizaje\n",
    "            _, c = sess.run([paso_entrenamiento, costo], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            #  Calcula el costo del minibatch y lo agrega al costo total\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        # Muestra los resultados\n",
    "        if epoch % display_step == 0:\n",
    "            print ((\"Epoch: \" + str(epoch)).ljust(20)\n",
    "                   + (\"Costo: \" + str(avg_cost)))\n",
    "    \n",
    "    #  Guarda la sesión en el archivo rnn2.cptk\n",
    "    saver.save(sess, archivo_modelo)\n",
    "    \n",
    "    print(\"Se acabaron los epochs, saliendo de la sesión de tensorflow.\")\n",
    "\n",
    "prediction_correcta = tf.equal(tf.argmax(estimado, 1), tf.argmax(y, 1))\n",
    "\n",
    "precision = tf.reduce_mean(tf.cast(prediction_correcta, \"float\"))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, archivo_modelo)\n",
    "    porcentaje_acierto = sess.run(precision, feed_dict={x: mnist.test.images,\n",
    "                                                        y: mnist.test.labels})\n",
    "    print(\"Precisión: {}\".format(porcentaje_acierto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
